{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling Data\n",
    "\n",
    "This notebook gives a brief introduction to a fundamental question that is rarely covered in ML courses: How to label data. In real life, it's rare that your datasets come pre-labeled for you. You have to do it yourself, which naively involves somebody going through and labeling *every single example* by hand. Sure, human labels are great and often ideal, but in practice it's impossible to do so for reasons of time and cost. It often falls on the data scientist or engineer to label the data himself. This begs the question, then. How can you label data to be \"good enough\" to build reliable ML models with a minimal amount of effort and resources?\n",
    "\n",
    "The topic of *weakly supervised learning* addresses this question by considering way to combine various data labeling strategies together to produce \"good enough\" labels. It may be the case that we can label some examples algorithmically using various rules-based functions, and combine those algorithmic labels with a subset of human-labeled examples (if we have them), and then train a model on *all* of these labels to predict the best choice for each example.\n",
    "\n",
    "This may seem confusing, but we'll go through an example below using a previous dataset: The spam classification dataset from the notebook on text classification. We will pretend that we don't have labels for that dataset, except a small subset (10% of the data), and our job is to use weak supervision to generate labels to do ML. We'll also compare them at the end with the ground truth labels (since we have them), though keep in mind that if you're using this technique you wouldn't be able to do this in real life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
