{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d84ebe",
   "metadata": {},
   "source": [
    "# Neural Networks: Part 2\n",
    "\n",
    "In part 2 of this neural networks series of tutorials we'll go into some more advanced ideas that make it easier to train deeper neural networks. We'll discuss concepts like regularization, normalization, activation functions, batches and dataloaders, and stochastic optimizers. We won't discuss new architectures yet, and just continue to stick with MLPs for now. Recall that an MLP is any neural network whose blocks are composed of linear layers + activation functions.\n",
    "\n",
    "We'll again load the usual libraries we'll need, set a seed, and set a device for those who'd prefer to work on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e6b8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1220ae6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8ae62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a08c66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2a4cb",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Recall from past tutorials that in practice it's usually bad to have zero loss or 100% accuracy on the training dataset, because it may mean you're just memorizing your data instead of discovering the underlying patterns in the data. This phenomenon is called **overfitting**. To help detect and prevent overfitting it's customary to keep a hold-out sample of dataset that the model doesn't get trained on, and use that dataset to evaluate the performance of the model. Such hold-out datasets are called the **test set** or the **validation set**. A model trained on a training set that also performs well on the test set is said to **generalize**, just a fancy way of saying the model isn't just memorizing the data but actually learning its underlying patterns really well.\n",
    "\n",
    "Let's look at an example of what overfitting might look like on a simple dataset, and then we'll discuss some deep learning based techniques to prevent it. Let's use the `make_blog` function to create a binary classification dataset of 800 samples with 2 features. We'll then split it up into training and test sets using the sklearn `train_test_split` function. We then plot the data with each axis being a feature $x_1, x_2$ and each color being the class 0 or 1. The test set data is deliberately emphasized in the plot, with the fainter data being the training set data. Notice that the negative samples (the 0 classes) overlap a good bit with the positive samples (the 1 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ec459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d963d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ab8b7e",
   "metadata": {},
   "source": [
    "Let's now try to train a neural network with 2 hidden layers each of 100 neurons to fit this training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fbc92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=500, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=500, out_features=500, bias=True)\n",
       "  (5): Sigmoid()\n",
       "  (6): Linear(in_features=500, out_features=1, bias=True)\n",
       "  (7): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train).float().to(device)\n",
    "y_train = torch.tensor(y_train).float().to(device).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test).float().to(device)\n",
    "y_test = torch.tensor(y_test).float().to(device).reshape(-1, 1)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_hidden = 100\n",
    "num_targets = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(num_features, num_hidden),\n",
    "    nn.Sigmoid(),\n",
    "#     nn.Linear(num_hidden, num_hidden),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(num_hidden, num_hidden),\n",
    "#     nn.Sigmoid(),\n",
    "    nn.Linear(num_hidden, num_targets),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2e261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dc939409444f0795525bc561c601a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "num_iters = 100_000\n",
    "for i in tqdm(range(num_iters)):\n",
    "    # training\n",
    "    opt.zero_grad()\n",
    "    yhat = model(X_train)\n",
    "    loss = loss_fn(yhat, y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # inference\n",
    "    yhat = model(X_test)\n",
    "    test_loss = loss_fn(yhat, y_test)\n",
    "    if i % (num_iters // 10) == 0:\n",
    "        print(f'iter = {i} \\t\\t train loss = {loss} \\t\\t test loss = {test_loss}')\n",
    "print(f'iter = {i} \\t\\t train loss = {loss} \\t\\t test loss = {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# num_iters = 1000\n",
    "# for i in range(num_iters):\n",
    "#     opt.zero_grad()\n",
    "#     yhat = model(X)\n",
    "#     loss = loss_fn(yhat, y)\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     if i % 100 == 0:\n",
    "#         print(f'iter = {i} \\t\\t loss = {loss}')\n",
    "# print(f'iter = {i} \\t\\t loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30352030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
