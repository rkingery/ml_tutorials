{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End ML (Part 2): Data Cleaning and Feature Engineering\n",
    "\n",
    "This is the second part of the series of tutorials on data wrangling. In the first part we started the process of data wrangling on the Titanic dataset, focusing on the EDA aspects. Recall that the point of doing EDA first was to give us a feel of the data. How big the data is; how many samples; how many features; problematic problems like missing values, duplicate samples, and useless features; etc. To see the results of our EDA on the Titatic dataset see that previous tutorial.\n",
    "\n",
    "In this next part we'll focus on data cleaning and feature engineering. The goal of data cleaning is to take the messy raw data we have and transform it into quality data that an ML model can train on. The goal of feature engineering is to identify which raw features from the raw data might be useful, and where feasible create potentially useful new derived features from them.\n",
    "\n",
    "We'll continue on with the example Titanic dataset from before. Since the plan is to do data cleaning, feature engineering, and (finally) train a working model, I'll go ahead and load numpy, pandas, matplotlib, and various sklearn and imblearn functions we covered before for imbalanced classification. I'll also specify a seed since randomness will be used in this code, and finally port over a useful function I've used in prior tutorials for printing out various metrics during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y,yhat):\n",
    "    print('accuracy: ', round(accuracy_score(y, yhat), 4))\n",
    "    print('precision: ', round(precision_score(y, yhat), 4))\n",
    "    print('recall: ', round(recall_score(y, yhat), 4))\n",
    "    print('f1: ', round(f1_score(y, yhat), 4))\n",
    "    print('auc: ', round(roc_auc_score(y, yhat), 4))\n",
    "    print('confusion matrix:\\n', confusion_matrix(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering: First Pass\n",
    "\n",
    "Focusing on the Titanic dataset again, I'll again import the data from the URL below, verify that the output is what was expected, and drop the `PassengerId` column since EDA identified it as a useless feature (a duplicate index). Recall that the ultimate goal here is to build a model that can predict whether a passenger of the Titanic survived, given the other features present in this dataset.\n",
    "\n",
    "This time when loading the dataframe I'll call it `df_raw`, and then make a copy of it into the dataframe `df` that we'll clean up. This is a good idea to do because it's easy to screw something up when data cleaning, and so if you do it's worth having the raw data around to go back to and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df_raw = pd.read_csv(url)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start by cleaning up the non-string columns since they're the easiest to deal with. Going in left-to-right order, we first have the `Survived` column, which will end up being the labels for the ML problem. From the EDA, we know that there are no missing values here, and that the values are already encoded as binary integers with `0 = not survived` and `1 = survived`. That means this column is already good and we can move on.\n",
    "\n",
    "The next column is `Pclass`. Recall that `Pclass` takes on the integer values `1, 2, 3`. It has no missing values. And it is evidently a categorical variable representing what boarding class the passenger was in. The fact that `Pclass` is categorical means that there is no (far as we can tell) no natural ordering. With categorical data, it wouldn't make sense to say `1 < 2 < 3`. It would make about as much sense as saying `red < blue < orange`. We thus may want to encode this data differently, e.g. one-hot encodings or embeddings. I'll ignore this for now and come back to it later. Other than this issue, this column appears good to go as well.\n",
    "\n",
    "The next non-string column is `Sex`. Recall that this column (as the name implies) represents passenger sex, and is encoded as `male`, `female`. This feature is obviously categorical. There are no missing values in this column. To deal with the encodings, I'll for now map `0 = male` and `1 = female`, though we may want to use a different categorical encoding later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    0  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  \n",
       "0         A/5 21171   7.2500   NaN        S  \n",
       "1          PC 17599  71.2833   C85        C  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3            113803  53.1000  C123        S  \n",
       "4            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sex'] = df['Sex'].replace(to_replace='male', value=0)\n",
    "df['Sex'] = df['Sex'].replace(to_replace='female', value=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the `Age` column. Recall this is a numerical feature with ages ranging roughly 0-80 years in not-necessarily-integer values (some passengers gave their age in years and months evidently). Also recall that indeed this column *does* have missing values. It looks like there are 177 of 891 missing values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the missing value question first. With missing values present in a column you generally have a few options:\n",
    "\n",
    "1. Drop the rows with missing values in that column. We could do this, and it may make since if there or only a handful missing. But 177 is a lot (almost 20% of the data). Dropping 20% of the data just doesn't seem like a great idea.\n",
    "2. Impute the missing values with an agreed upon value. But what value? We could just pick something random like 0 or -1 or 999 or whatever, but then we skew the distribution since this feature is continuous. A better idea would be to impute it with some kind of average value in the feature distribution, e.g. the column's mean, median, or mode. This is equivalent to saying \"I don't know what this value is, and without any better info my best guess is the average from that column\".\n",
    "3. Build a model to learn what to impute with. That is, you train a model that can take in as an example a row containing all features but that feature with missing values, and try to predict what that missing value should be,  given what the values in the other features are. This can be thought of as building a prior, where you ask \"what is my best guess for this missing value, given that I know what the values in the other features are for this example\". You can get a smarter imputer this way. However, it can be time consuming to build such a model, and the gains usually aren't helpful enough in practice to justify the effort. If you're interested in an easier way to do this, checkout sklearn's imputation approaches [here](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "For simplicity, and because it's the most common way to deal with missing values in practice, I'll go with approach (2) for this tutorial. It's also become useful to create a *new* binary feature to keep track of *which* values in a given column are missing. The reason is that there may in fact be some information in the fact that those values are missing, and maybe this is information the ML model might find useful in learning to make predictions.\n",
    "\n",
    "Focusing on the `Age` column, we can fill in missing values with a fixed value using the `df.fillna` method. I'll fill in the missing values with the *mean* age of 29.699 years. Doing this, we can see that there are no no missing values in that column. Also note a consequence of imputing with the mean is that the mean of that column won't change. Why? Adding more values of the mean to a dataset never changes the mean (prove it). It *will* affect the spread of the data though, making it generally go down.\n",
    "\n",
    "Notice I also create the new feature `Age_missing`, which is a binary feature where `0 = not missing` and `1 = missing`. I'll do this for every missing value column below as well. It's also good to check that `Age_missing` does indeed have 177 `1` values as well.\n",
    "\n",
    "After imputing missing values we'll go ahead and normalize the column as well by dividing by the max age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  Age_missing  \n",
       "0      0         A/5 21171   7.2500   NaN        S            0  \n",
       "1      0          PC 17599  71.2833   C85        C            0  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S            0  \n",
       "3      0            113803  53.1000  C123        S            0  \n",
       "4      0            373450   8.0500   NaN        S            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = df['Age'].mean()\n",
    "max_age = df['Age'].max()\n",
    "df['Age_missing'] = df['Age'].isna().astype(int)\n",
    "df['Age'] = df['Age'].fillna(value=mean_age)\n",
    "df['Age'] /= max_age\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Age_missing'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `Age` is now good to go. It's a numerical datatype with all missing values imputed, and the values have been normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next non-string column is `Fare`. This is a numerical feature with no missing values. Let's take a look at the range of fare values first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    891.000000\n",
       "mean      32.204208\n",
       "std       49.693429\n",
       "min        0.000000\n",
       "25%        7.910400\n",
       "50%       14.454200\n",
       "75%       31.000000\n",
       "max      512.329200\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Fare'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the fares are quite separated from each other, with a min of 0, median of 14.45, and a max all the way up at 512, which is well above the 75% percentile value of 31.\n",
    "\n",
    "When you have continuous data that's heavily skewed like this, it's often a good idea to take the log of the data before normalizing it. This ensures that spreads that when you have a very uneven distribution like `Fare` that the values get more equitably distributed, which can improve.\n",
    "\n",
    "**Note:** When taking the log of a set of data on a computer, it's usually a good idea to make sure all the terms are positive, and then add a small term to the function argument to prevent divisions by zero. This prevents getting weird NaN or infinity values, which can cause problems with analysis or machine learning results. In this case, I'll use a value of `eps = 1`. It's not small, but it doesn't matter for stuff like renormalizing data. A value of 1 is just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1\n",
    "df['Fare'] = np.log(df['Fare'] + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fare, min_fare = df['Fare'].max(), df['Fare'].min()\n",
    "df['Fare'] =  (df['Fare'] - min_fare) / (max_fare - min_fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    891.000000\n",
       "mean       0.474649\n",
       "std        0.155273\n",
       "min        0.000000\n",
       "25%        0.350464\n",
       "50%        0.438698\n",
       "75%        0.555325\n",
       "max        1.000000\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Fare'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, we have `Embarked`. From the EDA it looks like it's a categorical feature with 3 values `S, C, Q`. It also has missing values, but only 2 of them. Filling those in makes sense. Since this feature is categorical, you'd want to fill it in with the *mode* (the most frequently occuring value). In this case that mode is `S`. \n",
    "\n",
    "Next, we'll want to encode the categories with numbers, as ML models can't handle strings. To do this I'll perform the following mapping: `0 = S`, `1 = C`, `2 = Q`.\n",
    "\n",
    "Since there are only 2 missing values I won't bother to create a missing column here. It's just too sparse to matter. Thus, once we've done the imputing and format conversions we're done with this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S      644\n",
       "C      168\n",
       "Q       77\n",
       "NaN      2\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Embarked'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_embarked = df['Embarked'].mode().item()\n",
    "df['Embarked'] = df['Embarked'].fillna(value=mode_embarked)\n",
    "df['Embarked'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket      Fare Cabin  Embarked  Age_missing  \n",
       "0      0         A/5 21171  0.338125   NaN         0            0  \n",
       "1      0          PC 17599  0.685892   C85         1            0  \n",
       "2      0  STON/O2. 3101282  0.350727   NaN         0            0  \n",
       "3      0            113803  0.639463  C123         0            0  \n",
       "4      0            373450  0.352955   NaN         0            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Embarked'] = df['Embarked'].replace(to_replace='S', value=0)\n",
    "df['Embarked'] = df['Embarked'].replace(to_replace='C', value=1)\n",
    "df['Embarked'] = df['Embarked'].replace(to_replace='Q', value=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two columns we'll look at are `SigSp` and `Parch` together. Recall that these columns both take on positive integer values, and have no missing values present.\n",
    "\n",
    "Since both of these columns seem to refer to how many family members somebody has, as `SigSp` is the number of siblings and `Parch` is the number of parents. It may seem sensible that having a family is a decent predictor of survival. Thus, maybe we should create a new feature `Family` that sums these two columns together. We'll also create another feature `Alone` that just asks whether that person has any family. These could be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Family</th>\n",
       "      <th>Alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket      Fare Cabin  Embarked  Age_missing  Family  \\\n",
       "0      0         A/5 21171  0.338125   NaN         0            0       1   \n",
       "1      0          PC 17599  0.685892   C85         1            0       1   \n",
       "2      0  STON/O2. 3101282  0.350727   NaN         0            0       0   \n",
       "3      0            113803  0.639463  C123         0            0       1   \n",
       "4      0            373450  0.352955   NaN         0            0       0   \n",
       "\n",
       "   Alone  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Family'] = df['SibSp'] + df['Parch']\n",
    "df['Alone'] = (df['Family'] == 0).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for all the non-string columns. Let's see what we can do with just the columns we have.\n",
    "\n",
    "## Getting a Baseline\n",
    "\n",
    "What we'll first do then is take those columns and build our inputs from those, using those to train what we'll call a **baseline** model. A baseline is a quick, dirty model done with minimal work that can tell you how well you can do without trying to hard. The baseline is often the first model you want to try to beat. If the baseline is good enough, no need to waste time on more feature engineering or model iterations. If not, you can go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the data arrays `X_baseline` and `y`. For the inputs `X_baseline`, we'll only use the columns we've already processed. For the outputs `y` we'll use the `Survived` column as is. After creating these, we'll go ahead and split off the training and test sets.\n",
    "\n",
    "Note that in defining the training and test data I do it slightly different here than usual. Instead of splitting the training and test data directly, I instead split on their indexes and then index them out to get the data. The reason I do this is because we'll train more models on different features below, and it's good to do an objective comparison between the different approaches. If you sample different training and test data at each iteration, you don't necessarily know if one set of models is better than another.\n",
    "\n",
    "**Note:** That said, there's a caveat. You *must* be cognizant of the fact that it's easy to overfit your test set this way if you're not careful. Reason being you're training a bunch of models, tweaking the data, training a bunch more models, etc. When you do that it's easy to accidentally overfit your test set because you find yourself optimizing on test set metrics and pumping them up. Thus, be careful when you do this. It may be good practice to hold out a third *validation set* if you wish to do extensive data and model tuning like I'm doing here. Tune on the validation set, and only evaluate against your test set at the very very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 9), (891,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_columns = ['Sex', 'Age', 'Age_missing', 'Fare', 'Embarked', 'SibSp', 'Parch', 'Family', 'Alone']\n",
    "\n",
    "X_baseline = df[baseline_columns].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_baseline.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712,), (179,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train, idx_test = train_test_split(np.arange(len(y)), test_size=0.2, random_state=seed)\n",
    "idx_train.shape, idx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_baseline = X_baseline[idx_train]\n",
    "y_train = y[idx_train]\n",
    "X_test_baseline = X_baseline[idx_test]\n",
    "y_test = y[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying an actual model, let's see how well we can do just guessing the majority class. Since the labels are imbalanced, we'd do better than 50% accuracy with this very naive approach. How well can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6161616161616161"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_accuracy = (y == 0).sum() / len(y)\n",
    "naive_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like around 61.6%. This means if we're getting about 62%, we're not really doing any better than random. If we do worse than 62%, something weird is probably going on.\n",
    "\n",
    "Let's train the models now. The two I'll train are a random forest and a logistic regression model. Note the logistic regression model used is `LogisticRegressionCV`, which is similar to `LogisticRegression`, but does a hyperparameter search for you so it's giving you the highest-accuracy model from its search.\n",
    "\n",
    "Looking at the output from either one we can observe a few things: From the metrics, we can see the label-skewing (more non-survivors than survivors) makes accuracy appear deceptively high. Precision is good, indicating false positives (non-survivors classified as survivors) isn't too bad. The recall looks a bit worse, indicating the model had more problems with false negatives (survivors classified as non-survivors). The AUC is reasonably good as well.\n",
    "\n",
    "Not bad for a quick and dirty first go at it. Let's look at the feature importance plot for the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8101\n",
      "precision:  0.7123\n",
      "recall:  0.8\n",
      "f1:  0.7536\n",
      "auc:  0.8079\n",
      "confusion matrix:\n",
      " [[93 21]\n",
      " [13 52]]\n"
     ]
    }
   ],
   "source": [
    "rf_model_baseline = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_model_baseline.fit(X_train_baseline, y_train)\n",
    "\n",
    "yhat = rf_model_baseline.predict(X_test_baseline)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8045\n",
      "precision:  0.75\n",
      "recall:  0.6923\n",
      "f1:  0.72\n",
      "auc:  0.7804\n",
      "confusion matrix:\n",
      " [[99 15]\n",
      " [20 45]]\n"
     ]
    }
   ],
   "source": [
    "lr_model_baseline = LogisticRegressionCV(random_state=seed)\n",
    "lr_model_baseline.fit(X_train_baseline, y_train)\n",
    "\n",
    "yhat = lr_model_baseline.predict(X_test_baseline)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the feature importances for both models to see if they make sense. Let's look at the logistic regression model since its feature importances are easier to understand (they're just the absolute value of the model coefficients). It appears the model treats `Alone`, `Sex`, and `Fare` as pretty important. This makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_feature_importances(model):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return np.abs(model.coef_.flatten())\n",
    "\n",
    "def plot_feature_importances(model, feature_names=None, top_k=10, sort=True, **kwargs):\n",
    "    feature_importances = get_feature_importances(model)\n",
    "    idxs = feature_importances.argsort()[::-1] if sort else range(len(feature_importances))\n",
    "    feature_names = np.range(len(feature_importances)) if feature_names is None else np.array(feature_names)\n",
    "    feature_importances = feature_importances[idxs][:top_k]\n",
    "    feature_names = feature_names[idxs][:top_k]\n",
    "    plt.barh(feature_names[::-1], feature_importances[::-1], color='r', align='center')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "plot_feature_importances(lr_model_baseline, feature_names=baseline_columns, top_k=len(baseline_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In real life ML engineering, depending on business objectives and priorities, you might stop here and opt not to continue to feature engineer or improve the models. The reason is you often have a set of metrics thresholds indicating what's \"good enough\". Once you reach those thresholds, it often doesn't matter in a practical sense whether you can improve results. It's \"good enough\" and you can move onto other things. Odds are that all the feature engineering we're about to do will only improve the models by a percent or two (if even that), meaning you should really justify to yourself if the considerable extra development effort (both training and production) is worth it to you before doing so. It may make sense in research or in Kaggle where you're trying to hit a state of the art, but in most other cases other priorities are often as or more important than your model metrics. Think carefully.\n",
    "\n",
    "## Data Cleaning and Feature Engineering: Second Pass\n",
    "\n",
    "Let's now go back and see if we can improve results by feature engineering a little more. Let's look at the more complicated string columns that we've yet to touch and see if we can get some useful features out of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first such column we have is `Name`. Let's first look at a sample of these values. Printing the first 10 names we can see some patterns. First, names by and large tend to be in the order `last name, title, first name, middle name, alias`. Outside of that the strings don't appear to be that consistently formatted.\n",
    "\n",
    "Since we're trying to predict survival, it also *might* be work printing out some examples of `Name` strings of those who survived vs those who didn't, and see if there's a pattern we can maybe exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              Braund, Mr. Owen Harris\n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
       "2                               Heikkinen, Miss. Laina\n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
       "4                             Allen, Mr. William Henry\n",
       "5                                     Moran, Mr. James\n",
       "6                              McCarthy, Mr. Timothy J\n",
       "7                       Palsson, Master. Gosta Leonard\n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
       "2                                Heikkinen, Miss. Laina\n",
       "3          Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
       "8     Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n",
       "9                   Nasser, Mrs. Nicholas (Adele Achem)\n",
       "10                      Sandstrom, Miss. Marguerite Rut\n",
       "11                             Bonnell, Miss. Elizabeth\n",
       "15                     Hewlett, Mrs. (Mary D Kingcome) \n",
       "17                         Williams, Mr. Charles Eugene\n",
       "19                              Masselmani, Mrs. Fatima\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'][df['Survived'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               Braund, Mr. Owen Harris\n",
       "4                              Allen, Mr. William Henry\n",
       "5                                      Moran, Mr. James\n",
       "6                               McCarthy, Mr. Timothy J\n",
       "7                        Palsson, Master. Gosta Leonard\n",
       "12                       Saundercock, Mr. William Henry\n",
       "13                          Andersson, Mr. Anders Johan\n",
       "14                 Vestrom, Miss. Hulda Amanda Adolfina\n",
       "16                                 Rice, Master. Eugene\n",
       "18    Vander Planke, Mrs. Julius (Emelia Maria Vande...\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'][df['Survived'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pattern appears very clear in those who survived vs didn't: Those with titles like `Mr.` or `Master.` seem less likely to have survived than those with titles like `Mrs.` or `Miss.`. This is yet another reminder that survival on the Titanic strongly correlated with sex, something we already saw above in the baseline model. \n",
    "\n",
    "But, there may be something else to keeping these titles and making features out of them. So let's do that. I'm going to take the `Name` column and extract out of it a new column `Titles`. Let's first write some code to extract out the title for each column, and then show the value counts of those, so we can see which titles are worth keeping vs lumping together into a `Rare` group.\n",
    "\n",
    "The way I'll extract titles is by using the regular expression (\"regex\") string `' ([A-Za-z]+)\\.'`. If you're not familiar with regexes, this particular regex string says to take the input string from `Name`, match with any substring that\n",
    "- doesn't come first in the input string,\n",
    "- contains one or more alphabetical characters, possibly capitalized,\n",
    "- ends in a period.\n",
    "\n",
    "This makes sense for this particular problem of extracting titles because the titles aren't written first in the string, and always contain alphabetical characters followed by a period. After matching with substrings of this form, the regex will extract the alphabetical text *before* the period, and then show that (which we'll eventually dump into a new `Titles` column).\n",
    "\n",
    "Looking at the value counts, we can see that the set of titles `Mr, Miss, Mrs, Master` all occur over 10 times in the dataset, and the rest look to be rare. We'll map all of these rare titles into their own `Rare` title class and keep the rest separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr          517\n",
       "Miss        182\n",
       "Mrs         125\n",
       "Master       40\n",
       "Dr            7\n",
       "Rev           6\n",
       "Major         2\n",
       "Col           2\n",
       "Mlle          2\n",
       "Mme           1\n",
       "Ms            1\n",
       "Capt          1\n",
       "Lady          1\n",
       "Jonkheer      1\n",
       "Don           1\n",
       "Countess      1\n",
       "Sir           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'].str.extract(' ([A-Za-z]+)\\.').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr        517\n",
       "Miss      182\n",
       "Mrs       125\n",
       "Master     40\n",
       "Rare       27\n",
       "Name: Titles, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = ['Mr', 'Miss', 'Mrs', 'Master', 'Rare']\n",
    "df['Titles'] = df['Name'].str.extract(' ([A-Za-z]+)\\.')\n",
    "df['Titles'] = df['Titles'].apply(lambda x: x if x in titles else 'Rare')\n",
    "df['Titles'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last move will be to make `Titles` a good feature by mapping its values to integers and treating it as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    517\n",
       "1    182\n",
       "2    125\n",
       "3     40\n",
       "4     27\n",
       "Name: Titles, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_dict = {title:idx for (idx,title) in enumerate(titles)}\n",
    "df['Titles'] = df['Titles'].apply(lambda x: titles_dict[x])\n",
    "df['Titles'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Family</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket      Fare Cabin  Embarked  Age_missing  Family  \\\n",
       "0      0         A/5 21171  0.338125   NaN         0            0       1   \n",
       "1      0          PC 17599  0.685892   C85         1            0       1   \n",
       "2      0  STON/O2. 3101282  0.350727   NaN         0            0       0   \n",
       "3      0            113803  0.639463  C123         0            0       1   \n",
       "4      0            373450  0.352955   NaN         0            0       0   \n",
       "\n",
       "   Alone  Titles  \n",
       "0      0       0  \n",
       "1      0       2  \n",
       "2      1       1  \n",
       "3      0       2  \n",
       "4      1       0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column to attend to is `Ticket`. Let's again look at what these values tend to look like and see if there's a pattern. It looks like these are mostly strings of numbers. Some of them also have an alphanumeric prefix like `A/5` or `STON/02.`. Looking at the value counts, a small handful of ticket numbers are repeated, but the vast majority only have one count.\n",
    "\n",
    "Looking at the numbers, it's difficult to say what they refer to. If they're like most ticket numbers today they're just arbitrary strings of numbers with no clear meaning. Maybe there's something to saving the prefixes though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           A/5 21171\n",
       "1            PC 17599\n",
       "2    STON/O2. 3101282\n",
       "3              113803\n",
       "4              373450\n",
       "5              330877\n",
       "6               17463\n",
       "7              349909\n",
       "8              347742\n",
       "9              237736\n",
       "Name: Ticket, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Ticket'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347082      7\n",
       "CA. 2343    7\n",
       "1601        7\n",
       "3101295     6\n",
       "CA 2144     6\n",
       "           ..\n",
       "9234        1\n",
       "19988       1\n",
       "2693        1\n",
       "PC 17612    1\n",
       "370376      1\n",
       "Name: Ticket, Length: 681, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on the idea that the prefixes might have meaning, let's split off that part of the string and print out only the rows that have those prefixes (which I determine by checking if the string split contains at least 2 elements and dropping the rest). It looks like 226 of 891 passengers had a ticket with a prefix. Of those, it looks like 60 passengers had a `PC` prefix, 27 a `C.A.` prefix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PC            60\n",
       "C.A.          27\n",
       "STON/O        12\n",
       "A/5           10\n",
       "W./C.          9\n",
       "CA.            8\n",
       "SOTON/O.Q.     8\n",
       "A/5.           7\n",
       "SOTON/OQ       7\n",
       "STON/O2.       6\n",
       "CA             6\n",
       "F.C.C.         5\n",
       "C              5\n",
       "SC/PARIS       5\n",
       "S.O.C.         5\n",
       "SC/Paris       4\n",
       "A/4.           3\n",
       "S.O./P.P.      3\n",
       "SC/AH          3\n",
       "PP             3\n",
       "A/4            3\n",
       "P/PP           2\n",
       "S.C./PARIS     2\n",
       "SOTON/O2       2\n",
       "A.5.           2\n",
       "WE/P           2\n",
       "A./5.          2\n",
       "A/S            1\n",
       "SO/C           1\n",
       "F.C.           1\n",
       "S.W./PP        1\n",
       "SW/PP          1\n",
       "SC             1\n",
       "SCO/W          1\n",
       "W/C            1\n",
       "Fa             1\n",
       "S.O.P.         1\n",
       "S.C./A.4.      1\n",
       "A4.            1\n",
       "S.P.           1\n",
       "W.E.P.         1\n",
       "C.A./SOTON     1\n",
       "Name: Ticket, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticket_prefixes = df['Ticket'].str.split().apply(lambda x: x[0] if len(x) >= 2 else None)\n",
    "ticket_prefixes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefix strings do look a little messy. There are a lot of prefixes that are probably the same (e.g. `A/5`, `A/5.`, `A./5.`, and `A.5.`). It may help to clean these up by stripping out the punctuation, and then comparing.\n",
    "\n",
    "This looks somewhat better. It looks like we have 5 values that occur more than 10 times. What I think I'll do is this: Create new category values for the top 5 most occuring prefixes, and map the rest to a new `Other` value. This will give us a new column with 6 categories. As most rows won't have a prefix, most of the time the value will be `Other`. But enough rows *do* have another value that there should be enough information in this column to use. This is *not* to say it'll do any good with improving the model, but it's something to try and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PC         60\n",
       "CA         41\n",
       "A5         21\n",
       "SOTONOQ    15\n",
       "STONO      12\n",
       "WC         10\n",
       "SCPARIS     7\n",
       "A4          7\n",
       "STONO2      6\n",
       "SOC         6\n",
       "FCC         5\n",
       "C           5\n",
       "SCParis     4\n",
       "WEP         3\n",
       "SCAH        3\n",
       "SOPP        3\n",
       "PP          3\n",
       "SOTONO2     2\n",
       "SWPP        2\n",
       "PPP         2\n",
       "FC          1\n",
       "AS          1\n",
       "SCOW        1\n",
       "SC          1\n",
       "SP          1\n",
       "Fa          1\n",
       "SOP         1\n",
       "SCA4        1\n",
       "CASOTON     1\n",
       "Name: Ticket, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticket_prefixes = ticket_prefixes.str.replace('/', '')\n",
    "ticket_prefixes = ticket_prefixes.str.replace('.', '')\n",
    "ticket_prefixes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other      742\n",
       "PC          60\n",
       "CA          41\n",
       "A5          21\n",
       "SOTONOQ     15\n",
       "STONO       12\n",
       "Name: Prefix, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes = ['PC', 'CA', 'A5', 'SOTONOQ', 'STONO', 'Other']\n",
    "df['Prefix'] = ticket_prefixes.apply(lambda x: x if x in prefixes else 'Other')\n",
    "df['Prefix'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now map these new `Prefix` values to integers and treat them as a proper category. Note by the value counts this feature is highly imbalanced, so this may be kind of a crap shoot with improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    742\n",
       "0     60\n",
       "1     41\n",
       "2     21\n",
       "3     15\n",
       "4     12\n",
       "Name: Prefix, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_dict = {prefix:idx for (idx,prefix) in enumerate(prefixes)}\n",
    "df['Prefix'] = df['Prefix'].apply(lambda x: prefix_dict[x])\n",
    "df['Prefix'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing that might be useful to do is to keep track of ticket counts, i.e. the number of time each one ticket number occurs. To do this we'll create a continuous `Ticket_freq` column and normalize it by the max count.\n",
    "\n",
    "While the vast majority of ticket numbers only occur once, it's interesting that there are so many copies of ticket numbers. Counterfeit tickets maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket_freq'] = df.groupby('Ticket')['Ticket'].transform('count')\n",
    "max_freq = df['Ticket_freq'].max()\n",
    "df['Ticket_freq'] /= max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Family</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Prefix</th>\n",
       "      <th>Ticket_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket      Fare Cabin  Embarked  Age_missing  Family  \\\n",
       "0      0         A/5 21171  0.338125   NaN         0            0       1   \n",
       "1      0          PC 17599  0.685892   C85         1            0       1   \n",
       "2      0  STON/O2. 3101282  0.350727   NaN         0            0       0   \n",
       "3      0            113803  0.639463  C123         0            0       1   \n",
       "4      0            373450  0.352955   NaN         0            0       0   \n",
       "\n",
       "   Alone  Titles  Prefix  Ticket_freq  \n",
       "0      0       0       2     0.142857  \n",
       "1      0       2       0     0.142857  \n",
       "2      1       1       5     0.142857  \n",
       "3      0       2       5     0.285714  \n",
       "4      1       0       5     0.142857  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is `Cabin`, which like `Name` and `Ticket` appears to be a high-dimensional categorical column, but unlike those it has lots of missing values. A full 77% of the data is missing! How should you impute this one? Let's step back and see if we can extract a feature here.\n",
    "\n",
    "Printing the first few values, it looks like the cabin value is formatted letter-number, sometimes having multiple such values per passenger. Let's split these up by letter and number and see if we can spot a pattern in each separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1             C85\n",
       "3            C123\n",
       "6             E46\n",
       "10             G6\n",
       "11           C103\n",
       "21            D56\n",
       "23             A6\n",
       "27    C23 C25 C27\n",
       "31            B78\n",
       "52            D33\n",
       "Name: Cabin, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cabin'].dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN            687\n",
       "C23 C25 C27      4\n",
       "G6               4\n",
       "B96 B98          4\n",
       "C22 C26          3\n",
       "              ... \n",
       "E34              1\n",
       "C7               1\n",
       "C54              1\n",
       "E36              1\n",
       "C148             1\n",
       "Name: Cabin, Length: 148, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cabin'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I look at some useful statistics that might be worth looking into for creating a feature here:\n",
    "- From the printouts above it looks like some passengers have multiple cabin numbers. How frequent is this? If this is a fairly common thing, maybe we should create a feature tracking this.\n",
    "- How often does each letter prefix in the cabin numbers appear? If they occur with enough frequency, it may be worth tracking them as a feature.\n",
    "- How often does each number suffix in the cabin numbers appear? Again, if they occur enough, may be worth tracking.\n",
    "\n",
    "Looking at these one by one. First, it looks like there are only 24 rows with multiple cabin numbers. That's like 2% of the data. Probably not worth keeping track of? This may be something to come back to later in model improvement iterations.\n",
    "\n",
    "Second, looking at the value counts of the cabin prefix letters, they seem to take on 8 possible values, with most occuring with enough frequency that it may be worth tracking this.\n",
    "\n",
    "Last, looking at the value counts of the cabin suffix numbers, there seem to be a lot of these, with none occuring at least 10 time. This suggest to me that this isn't worth keeping track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows with multiple cabins\n",
    "df['Cabin'].str.split().dropna().apply(lambda x: len(x) > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    59\n",
       "B    47\n",
       "D    33\n",
       "E    32\n",
       "A    15\n",
       "F    13\n",
       "G     4\n",
       "T     1\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cabin'].str.split().dropna().apply(lambda x: x[0][0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33     7\n",
       "6      6\n",
       "23     5\n",
       "22     5\n",
       "2      5\n",
       "      ..\n",
       "21     1\n",
       "12     1\n",
       "63     1\n",
       "14     1\n",
       "148    1\n",
       "Name: Cabin, Length: 92, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cabin'].str.split().dropna().apply(lambda x: x[0][1:] if x[0][1:] != '' else None).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this. We'll create a `Level` feature that extracts and stores the cabin letter for each passenger, binned into levels. We'll map `A`,`B`,`C` to `ABC`; `D`,`E` to `DE`; `F`,`G` to `FG`; and finally map the rest to `Other`.\n",
    "\n",
    "Since there are so many missing values, we need to deal with these as well. Since we can't drop them, I'll do the next simplest thing for now: I'll create a new `Level_missing` feature to track which values are missing, and then impute the missing values in `Level` with the mode `ABC`. This *does* risk grossly overweighting the importance of `ABC` relative to other levels, but hopefully the missing column combined with a model capable of learning complex correlations will help deal with this.\n",
    "\n",
    "Finally, I'll convert these into proper categorical features by mapping them to integers, and then drop the `Cabin` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    59\n",
       "B    47\n",
       "D    33\n",
       "E    32\n",
       "A    15\n",
       "F    13\n",
       "G     4\n",
       "T     1\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using x[0][0] to account for possibility of multiple cabin numbers in a row, selecting first one arbitrarily\n",
    "df['Level'] = df['Cabin'].fillna(value='?').str.split().apply(lambda x: x[0][0])\n",
    "df['Level'] = df['Level'].replace('?', np.nan)\n",
    "df['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    687\n",
       "0    204\n",
       "Name: Level_missing, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Level_missing'] = df['Level'].isna().astype(int)\n",
    "df['Level_missing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABC    0.636364\n",
       "DE     0.753846\n",
       "FG     0.588235\n",
       "Name: Level, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = {'A': 'ABC', 'B': 'ABC', 'C': 'ABC', 'D': 'DE', 'E': 'DE', 'F': 'FG', 'G': 'FG'}\n",
    "df['Level'] = df['Cabin'].str[0].map(levels)\n",
    "df['Level'] = df['Level'].apply(lambda x: x if x in levels.values() or pd.isna(x) else 'Other')\n",
    "df['Level'][df['Survived'] == 1].dropna().value_counts() / df['Level'].dropna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABC    809\n",
       "DE      65\n",
       "FG      17\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_level = df['Level'].mode().item()\n",
    "df['Level'] = df['Level'].replace(np.nan, mode_level)\n",
    "df['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    809\n",
       "4     65\n",
       "6     17\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_dict = {level:idx for (idx,level) in enumerate(levels.values())}\n",
    "df['Level'] = df['Level'].apply(lambda x: level_dict[x])\n",
    "df['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age_missing</th>\n",
       "      <th>Family</th>\n",
       "      <th>Alone</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Prefix</th>\n",
       "      <th>Ticket_freq</th>\n",
       "      <th>Level</th>\n",
       "      <th>Level_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex     Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    0  0.2750      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  0.4750      1   \n",
       "2                             Heikkinen, Miss. Laina    1  0.3250      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  0.4375      1   \n",
       "4                           Allen, Mr. William Henry    0  0.4375      0   \n",
       "\n",
       "   Parch            Ticket      Fare Cabin  Embarked  Age_missing  Family  \\\n",
       "0      0         A/5 21171  0.338125   NaN         0            0       1   \n",
       "1      0          PC 17599  0.685892   C85         1            0       1   \n",
       "2      0  STON/O2. 3101282  0.350727   NaN         0            0       0   \n",
       "3      0            113803  0.639463  C123         0            0       1   \n",
       "4      0            373450  0.352955   NaN         0            0       0   \n",
       "\n",
       "   Alone  Titles  Prefix  Ticket_freq  Level  Level_missing  \n",
       "0      0       0       2     0.142857      2              1  \n",
       "1      0       2       0     0.142857      2              0  \n",
       "2      1       1       5     0.142857      2              1  \n",
       "3      0       2       5     0.285714      2              0  \n",
       "4      1       0       5     0.142857      2              1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're good to go. We've feature engineered all of our original input columns. We have an array of numerical data that we can do machine learning with.\n",
    "\n",
    "Let's start by looking at what happens if we train a model with the new features, but don't do any one-hot encoding of the categorical features.\n",
    "\n",
    "Looks like we're already doing pretty good. The F1 scores for both RF and LR have pulled up a few percent. Can we do still better by one-hot encoding, or by balancing the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 14)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_onehot_columns = baseline_columns + ['Titles', 'Prefix', 'Ticket_freq', 'Level', 'Level_missing']\n",
    "\n",
    "X_no_onehot = df[no_onehot_columns].values\n",
    "X_no_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_onehot = X_no_onehot[idx_train]\n",
    "X_test_no_onehot = X_no_onehot[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.838\n",
      "precision:  0.7571\n",
      "recall:  0.8154\n",
      "f1:  0.7852\n",
      "auc:  0.8331\n",
      "confusion matrix:\n",
      " [[97 17]\n",
      " [12 53]]\n"
     ]
    }
   ],
   "source": [
    "rf_model_no_onehot = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_model_no_onehot.fit(X_train_no_onehot, y_train)\n",
    "\n",
    "yhat = rf_model_no_onehot.predict(X_test_no_onehot)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8156\n",
      "precision:  0.7353\n",
      "recall:  0.7692\n",
      "f1:  0.7519\n",
      "auc:  0.8057\n",
      "confusion matrix:\n",
      " [[96 18]\n",
      " [15 50]]\n"
     ]
    }
   ],
   "source": [
    "lr_model_no_onehot = LogisticRegressionCV(random_state=seed)\n",
    "lr_model_no_onehot.fit(X_train_no_onehot, y_train)\n",
    "\n",
    "yhat = lr_model_no_onehot.predict(X_test_no_onehot)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now instead treat the categorical features as we \"should\", by one-hot encoding them first. Recall that one-hot encoding is a way of converting one feature of k categories `A1, A2, ..., Ak` into k features of 2 categories `is_A1, is_A2, ..., is_A3`. Essentially it creates a series of binary indicator that looks for whether a particular category is present, each being its own feature.\n",
    "\n",
    "I'll only one-hot encode the features we'd already decided were categorical, and that contain more than 2 categories. Note that if a feature only contains 2 categories you don't need to one-hot encode it. Why?\n",
    "\n",
    "We can easily one-hot encode features by using the `pd.get_dummies` function, passing in the columns that we want to one-hot encode, i.e. the categorical columns with more than 2 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>...</th>\n",
       "      <th>Titles_4</th>\n",
       "      <th>Prefix_0</th>\n",
       "      <th>Prefix_1</th>\n",
       "      <th>Prefix_2</th>\n",
       "      <th>Prefix_3</th>\n",
       "      <th>Prefix_4</th>\n",
       "      <th>Prefix_5</th>\n",
       "      <th>Level_2</th>\n",
       "      <th>Level_4</th>\n",
       "      <th>Level_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>C85</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>0.639463</td>\n",
       "      <td>C123</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>0.352955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived                                               Name  \\\n",
       "0            1         0                            Braund, Mr. Owen Harris   \n",
       "1            2         1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3         1                             Heikkinen, Miss. Laina   \n",
       "3            4         1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5         0                           Allen, Mr. William Henry   \n",
       "\n",
       "   Sex     Age  SibSp  Parch            Ticket      Fare Cabin  ...  Titles_4  \\\n",
       "0    0  0.2750      1      0         A/5 21171  0.338125   NaN  ...         0   \n",
       "1    1  0.4750      1      0          PC 17599  0.685892   C85  ...         0   \n",
       "2    1  0.3250      0      0  STON/O2. 3101282  0.350727   NaN  ...         0   \n",
       "3    1  0.4375      1      0            113803  0.639463  C123  ...         0   \n",
       "4    0  0.4375      0      0            373450  0.352955   NaN  ...         0   \n",
       "\n",
       "   Prefix_0  Prefix_1  Prefix_2  Prefix_3  Prefix_4  Prefix_5  Level_2  \\\n",
       "0         0         0         1         0         0         0        1   \n",
       "1         1         0         0         0         0         0        1   \n",
       "2         0         0         0         0         0         1        1   \n",
       "3         0         0         0         0         0         1        1   \n",
       "4         0         0         0         0         0         1        1   \n",
       "\n",
       "   Level_4  Level_6  \n",
       "0        0        0  \n",
       "1        0        0  \n",
       "2        0        0  \n",
       "3        0        0  \n",
       "4        0        0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_hot = pd.get_dummies(df, columns=['Pclass', 'Embarked', 'Titles', 'Prefix', 'Level'])\n",
    "df_one_hot.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 30)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_columns = [\n",
    "    'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Age_missing', 'Family', 'Alone', 'Ticket_freq', 'Level_missing', \n",
    "    'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_0', 'Embarked_1', 'Embarked_2', 'Titles_0', 'Titles_1', \n",
    "    'Titles_2', 'Titles_3', 'Titles_4', 'Prefix_0', 'Prefix_1', 'Prefix_2', 'Prefix_3', 'Prefix_4', 'Prefix_5', \n",
    "    'Level_2', 'Level_4', 'Level_6'\n",
    "]\n",
    "\n",
    "X_onehot = df_one_hot[one_hot_columns].values\n",
    "X_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = X_onehot[idx_train]\n",
    "X_test_onehot = X_onehot[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an RF and LR now, it looks like performance is slightly better or about the same as without the one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8547\n",
      "precision:  0.791\n",
      "recall:  0.8154\n",
      "f1:  0.803\n",
      "auc:  0.8463\n",
      "confusion matrix:\n",
      " [[100  14]\n",
      " [ 12  53]]\n"
     ]
    }
   ],
   "source": [
    "rf_model_onehot = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_model_onehot.fit(X_train_onehot, y_train)\n",
    "\n",
    "yhat = rf_model_onehot.predict(X_test_onehot)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8603\n",
      "precision:  0.8125\n",
      "recall:  0.8\n",
      "f1:  0.8062\n",
      "auc:  0.8474\n",
      "confusion matrix:\n",
      " [[102  12]\n",
      " [ 13  52]]\n"
     ]
    }
   ],
   "source": [
    "lr_model_onehot = LogisticRegressionCV(random_state=seed)\n",
    "lr_model_onehot.fit(X_train_onehot, y_train)\n",
    "\n",
    "yhat = lr_model_onehot.predict(X_test_onehot)\n",
    "get_scores(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's look at the logistic regression model's feature importances again to see if anything is different now. It looks like `Age`, `Sex`, and `Fare` are still important features, but some of the new one-hot features snuck in too. The top handful of new features appear to be `Prefix_4` (i.e. tickets with a `STON/O` prefix), `Title_3` (people title \"Master\", i.e. young boys), and `Title_0` (people title \"Mr\", i.e. older men). Since these two title features clearly correlate with `Sex` and `Age`, they're likely diminishing the true importance of the `Sex` `Age ` features as predictors of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature_importances(lr_model_onehot, feature_names=one_hot_columns, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we might choose to save the processed data for future use. We can do this using the pandas dataframe `to_csv` method, specifying the path where we want it to save. Note the use of the `pathlib.Path` object for specifying file paths. This is by far the most convenient way to work with paths in modern versions of python. The path we'll save to is `../resources/titanic_cleaned.csv`. Note that we won't actually use this saved file in the next tutorial, instead just importing our preprocessing as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path().cwd().parent / 'resources'\n",
    "df_one_hot.to_csv(path / 'titanic_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Since we've calculated a lot of scores on several different models, it's not a bad idea to put them all into a table so we can compare them against each other. We'll do that in the table below, which is set up to auto fill in the scores from above (even if we run it with a different seed). Cool, right? \n",
    "\n",
    "**Note:** Unfortunately, this table won't render correctly if you're viewing it from Github or the online NB viewer, as it requires that extensions be installed. To view it correctly, you'll need to download the notebook and run it locally, after installing the Jupyter [contrib extensions](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html) and enabling the python in markdown extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "round(accuracy_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)": "0.8045",
     "round(accuracy_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)": "0.8156",
     "round(accuracy_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)": "0.8603",
     "round(accuracy_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)": "0.8101",
     "round(accuracy_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)": "0.838",
     "round(accuracy_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)": "0.8547",
     "round(f1_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)": "0.72",
     "round(f1_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)": "0.7519",
     "round(f1_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)": "0.8062",
     "round(f1_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)": "0.7536",
     "round(f1_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)": "0.7852",
     "round(f1_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)": "0.803",
     "round(precision_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)": "0.75",
     "round(precision_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)": "0.7353",
     "round(precision_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)": "0.8125",
     "round(precision_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)": "0.7123",
     "round(precision_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)": "0.7571",
     "round(precision_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)": "0.791",
     "round(recall_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)": "0.6923",
     "round(recall_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)": "0.7692",
     "round(recall_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)": "0.8",
     "round(recall_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)": "0.8",
     "round(recall_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)": "0.8154",
     "round(recall_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)": "0.8154",
     "round(roc_auc_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)": "0.7804",
     "round(roc_auc_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)": "0.8057",
     "round(roc_auc_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)": "0.8474",
     "round(roc_auc_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)": "0.8079",
     "round(roc_auc_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)": "0.8331",
     "round(roc_auc_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)": "0.8463",
     "seed": "123"
    }
   },
   "source": [
    "|(seed={{seed}})|accuracy|precision|recall|F1|AUC|\n",
    "|---|---|---|---|---|---|\n",
    "|RF baseline|{{round(accuracy_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|{{round(precision_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|{{round(recall_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|{{round(f1_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|{{round(roc_auc_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|{{round(recall_score(y_test, rf_model_baseline.predict(X_test_baseline)),4)}}|\n",
    "|LR baseline|{{round(accuracy_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)}}|{{round(precision_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)}}|{{round(recall_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)}}|{{round(f1_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)}}|{{round(roc_auc_score(y_test, lr_model_baseline.predict(X_test_baseline)),4)}}|\n",
    "|RF no onehot|{{round(accuracy_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(precision_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(recall_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(f1_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(roc_auc_score(y_test, rf_model_no_onehot.predict(X_test_no_onehot)),4)}}|\n",
    "|LR no onehot|{{round(accuracy_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(precision_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(recall_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(f1_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)}}|{{round(roc_auc_score(y_test, lr_model_no_onehot.predict(X_test_no_onehot)),4)}}|\n",
    "|RF onehot|{{round(accuracy_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)}}|{{round(precision_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)}}|{{round(recall_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)}}|{{round(f1_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)}}|{{round(roc_auc_score(y_test, rf_model_onehot.predict(X_test_onehot)),4)}}|\n",
    "|LR onehot|{{round(accuracy_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)}}|{{round(precision_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)}}|{{round(recall_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)}}|{{round(f1_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)}}|{{round(roc_auc_score(y_test, lr_model_onehot.predict(X_test_onehot)),4)}}|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the LR onehot model is on the whole doing the best across all metrics. This suggests we should choose this model, right? Well, perhaps not. It turns out that the above scores are *very* sensitive to the choice of seed. I deliberately picked a seed (123) that gives \"high\" accuracies of 85-86%. Most other seeds will actually give lower accuracies, usually somewhere in the 79-86% range. Try changing the seed at the top of the notebook to a different number and re-running if you don't believe me! It'll almost certainly give much lower scores across the board.\n",
    "\n",
    "Given that our metrics have uncertainties on the order of 5% like this depending which seed we choose, how do we know which model is truly \"best\"? After all, in ML we usually want a model that works best on *all unseen* data, not particular \"lucky\" subsets of the data. The reason we're having this problem is because our dataset is \"small\". That may seem surprising, given that it has 891 examples, but *relative to the task* it's quite small. Predicting survival from this dataset turns out to be a surprisingly noisy task, which means to get stable, accurate estimates we'd need a lot more data.\n",
    "\n",
    "What can we do about this problem, given that we can't exactly get more Titanic data? The trick is to use model selection techniques like K-fold cross validation. These techniques will allow us to work better with small-ish, noisy data like Titanic. \n",
    "\n",
    "In the next tutorial, we'll use model selection techniques to help pick out the best model to use. We'll also talk a little bit about so-called ML Ops and how to deploy a simple Titanic survival prediction model to \"production\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "124px",
    "width": "411px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
