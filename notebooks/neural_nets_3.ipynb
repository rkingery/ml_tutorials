{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb86501",
   "metadata": {},
   "source": [
    "# Neural Networks (Part 3): Optimizers\n",
    "\n",
    "In part 3 of this neural networks series of tutorials we'll discuss topics around batching and optimizers:\n",
    "- Datasets and Dataloaders\n",
    "- Batching and Batch Sizes\n",
    "- Optimizers: SGD, Adam, RAdam, AdamW\n",
    "\n",
    "We'll again load the usual libraries we'll need, set a seed, and set a device for those who'd prefer to work on the GPU. We'll also continue to use our MLP creator function, our parameter counter, and our learning rate finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a052b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1156ae6f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416dc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5936ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ef0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_mlp(num_features, num_targets, hidden_sizes, p=0, normalization=None, activation=nn.ReLU()):\n",
    "    \"\"\"\n",
    "    Get an arbitrary MLP model with L=len(hidden_sizes)+1 layers\n",
    "    Optionally, can include a batchnorm and dropout in each layer, and change the activation function\n",
    "    \"\"\"\n",
    "    def get_leaf_layers(m):\n",
    "        \"\"\"Used to flatten the layers out so the model doesn't recursively nest\"\"\"\n",
    "        children = list(m.children())\n",
    "        if not children:\n",
    "            return [m]\n",
    "        leaves = []\n",
    "        for l in children:\n",
    "            leaves.extend(get_leaf_layers(l))\n",
    "        return leaves\n",
    "    hidden_sizes = [num_features] + hidden_sizes\n",
    "    n_layers = len(hidden_sizes)\n",
    "    model = nn.Sequential()\n",
    "    for l in range(n_layers-1):\n",
    "        linear = nn.Linear(hidden_sizes[l], hidden_sizes[l+1])\n",
    "        dropout = None if p == 0. else nn.Dropout(p)\n",
    "        layer_norm = None if normalization is None else normalization(hidden_sizes[l+1])\n",
    "        blocks = [linear, layer_norm, activation, dropout]\n",
    "        layer_block = nn.Sequential(*[layer for layer in blocks if layer is not None])\n",
    "        model = nn.Sequential(*([model] + [layer_block]))\n",
    "    output = nn.Sequential(nn.Linear(hidden_sizes[-1], num_targets))\n",
    "    model = nn.Sequential(*([model] + [output]))\n",
    "    model = nn.Sequential(*get_leaf_layers(model))\n",
    "    return model\n",
    "\n",
    "def lr_find(dataset, model, opt, loss_fn, batch_size=128, **kwargs):\n",
    "    from torch_lr_finder import LRFinder\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    lr_finder = LRFinder(model, opt, loss_fn, device=device)\n",
    "    lr_finder.range_test(dataloader, **kwargs)\n",
    "    lr_finder.plot()\n",
    "    lr_finder.reset()\n",
    "    \n",
    "def num_params(model):\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310fcd33",
   "metadata": {},
   "source": [
    "## Data Prep: MNIST\n",
    "\n",
    "For this tutorial we'll want to start going bigger with the data we're using. We'll start by taking baby steps, just using the MNIST dataset. Recall the MNIST dataset consists of 70k grayscale images of handwritten digits, each image of size 28x28 and being labeled with a digit 0-9 (so 10 multiclass classification with 10 classes). The openml link we're downloading the data from has the images in *flattened* form, meaning each 28x28 image gets flattened to a single vector of length 28*28=784. We can always turn the back into images by reshaping. Below is a plot of a random sample of MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cd3b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70000, 784]), torch.Size([70000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_mnist_data():\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    data = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "    X = data[0].astype(float)\n",
    "    X = MinMaxScaler(feature_range=(0,1)).fit_transform(X)\n",
    "    y = data[1].astype(int)\n",
    "    return torch.tensor(X).float(), torch.tensor(y).long()\n",
    "\n",
    "X,y = get_mnist_data()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f68c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(X, n=4):\n",
    "    from matplotlib import cm\n",
    "    idx = np.random.choice(len(X), n**2, replace=False)\n",
    "    _, ax = plt.subplots(nrows=n, ncols=n, figsize=(8, 8))\n",
    "    cnt = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i, j].imshow(X[idx[cnt]].reshape(28, 28), aspect='auto', cmap=cm.gray)\n",
    "            ax[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.show()\n",
    "    \n",
    "plot_images(X.cpu().numpy(), n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb2f05",
   "metadata": {},
   "source": [
    "We might as well go ahead and split the dataset up into training and test sets. Following the convention with this dataset, we'll take 60k images for training and 10k images for testing.\n",
    "\n",
    "While we're at it, let's go ahead and see how well a baseline, no frills classical model does classifying the images in this dataset. We'll use the random forest with the defaults (100 trees). Looks like it can already classify MNIST images with 96.8% accuracy! That's a pretty high bar to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a71b4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b4d6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9679"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=seed)\n",
    "model.fit(X_train.cpu().numpy(), y_train.cpu().numpy())\n",
    "model.score(X_test.cpu().numpy(), y_test.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1ef22",
   "metadata": {},
   "source": [
    "## Datasets, Dataloaders, and Batches\n",
    "\n",
    "So far we've been working with data the same way one might do in sklearn, having an input data array `X` and an output data array `y` that we keep track of separately. Pytorch natively prefers, however, that data examples `x,y` be grouped together in pairs, so that the dataset is a list of these input-output example pairs:\n",
    "```\n",
    "dataset = [(X[0], y[0]), (X[1], y[1]), ..., (X[N-1], y[N-1])]\n",
    "```\n",
    "Pytorch likes it this way because it makes it easier to convert datasets to dataloaders and work in batches, something we'll get to shortly. \n",
    "\n",
    "We can pretty easily create pytorch compatible datasets by creating a list of tuples for both our training and test sets. We'll call the training dataset `train_set` and the test dataset `test_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9417303",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(X_train[i], y_train[i]) for i in range(len(y_train))]\n",
    "test_set = [(X_test[i], y_test[i]) for i in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef555e15",
   "metadata": {},
   "source": [
    "With our data in this form we can talk about dataloaders, probably the most important data structure in pytorch. Basically put, a dataloader splits data up into **batches** according to a specified **batch size**. We'll deal with why we might want to do this in a second. First, the how.\n",
    "\n",
    "To convert a dataset into a dataloader, we use the `torch.utils.data.DataLoader` class, passing in the dataset, a batch size, and other optional arguments we'll cover as we go. Here, we'll convert our `train_set` and `test_set` objects into dataloaders `train_loader` and `test_loader`. For now we'll use a batch size of 32. We'll also pass in `shuffle=True`, which tells pytorch to shuffle the datasets randomly, a good idea for training models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d7f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d80c4",
   "metadata": {},
   "source": [
    "What exactly is a dataloader though? Basically, a dataloader takes your dataset and splits it into chunks each with `batch_size` many data examples. Each iteration of the dataloader then gives `X, y` pairs of shuffled examples, but `batch_size` many of them at a time. It does this until it gets through the whole dataset, at which point it starts over again. Note that by using the datasets approach above you're guaranteed each `x,y` pair will be the appropriate feature-target pair, i.e. the data samples will always shuffle together in input-output pairs.\n",
    "\n",
    "One subtlety to get used to is that pytorch dataloaders are python *generator* objects. Roughly speaking, a generator is a list that hasn't been instantiated. All you can do is grab the next item, iteratively, using `next(iter(dataloader))`. You can also loop over if using `for X, y in dataloader` and it'll work the usual way. Just remember that, as a generator, the `dataloader` object can't be indexed into without converting it into a list first.\n",
    " \n",
    "Here's an example of grabbing the next *batch* from a dataloader. Notice that the 0-dimension is size 32, which is our batch size. For inputs `X`, the 1-dimension is the feature number, in our case 784. For targets `y`, the 1-dimension (if any) will be the number of classes or targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0ecc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 784]), torch.Size([32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a0cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784]) torch.Size([32])\n",
      "torch.Size([32, 784]) torch.Size([32])\n",
      "torch.Size([32, 784]) torch.Size([32])\n",
      "torch.Size([32, 784]) torch.Size([32])\n",
      "torch.Size([32, 784]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for X, y in train_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12af2c",
   "metadata": {},
   "source": [
    "If you want the whole dataloader in memory at once you first have to convert it into a list using `list(dataloader)`. You usually will not want to do this though, as we'll see. It can be useful for telling us how many batches are in the dataset though. In our case, it looks like there are 1875 batches in the training set when using a batch size of 32. That makes sense, given that `32*1875=60,000` is our training set size. It won't always be exact like this. Sometimes you'll have leftover data in the last batch when the batch number and the dataset size don't divide evenly. This is true, for example, with the test set. There, `32*313=10,016`. But the test set size is 10,000. Evidently the last batch only contains `10,000-32*(313-1)=16` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d04b7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batches in training set: 1875\n",
      "# batches in test set: 313\n"
     ]
    }
   ],
   "source": [
    "print(f'# batches in training set: {len(list(train_loader))}')\n",
    "print(f'# batches in test set: {len(list(test_loader))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9e4e7",
   "metadata": {},
   "source": [
    "Why would we want to bother with all this batching stuff though when we can just work with the whole dataset at once? The simple reason is, for any reasonably large dataset, doing deep learning slows to a crawl if you try to feed in all the data at once. You'll find that it takes *forever* to do a forward pass of the network. Worse, you might (and often will) run out of memory either on the CPU or the GPU. \n",
    "\n",
    "For example, the MNIST dataset is considered \"small\" by today's deep learning standards. But if you try to pass all 70k images through any decent deep learning model at one time you'll probably slow your performance way down. Assuming 4 bytes per number, a 28x28 image will take up `4*28*28=3186` bytes, or 3.186 kB of memory. That means 70,000 images will take up 219,520,000 bytes, or 219 MB. That doesn't seem like a lot, but it's enough to slow down your performance a lot, especially when you're passing that much data through a model of millions of parameters for many iterations.\n",
    "\n",
    "A clever way around this problem is to not pass through *all* the data in one forward pass of the network, but to only pass through smaller batches of it at a time. This is what dataloaders allow you to do. They're an efficient way to feed data to your model in a way that keeps up system performance and avoids crashing your CPU/GPU.\n",
    "\n",
    "This isn't the only reason batching is a good idea though. It turns out that batching your data also acts as a form of regularization! Essentially, when you feed data through a neural network in batches, it will only update the gradients based on the data in *that particular batch*. But the data in any given batch is just a random subsample of the total dataset, meaning it's only a *guess* of what the best, true gradients should be. The smaller the batch size, the more random this estimate will be. A batch size of 1 will mean *a lot* of regularization and randomness during training, and a batch size of `N`, the dataset size, will mean no regularization or randomness at all.\n",
    "\n",
    "The introduction of the batch size thus means we've got a new hyperparameter to tune when training models. How should it be tuned? The emerging rule of thumb from experiments is that you want to pick the *largest* batch size that will still allow your data to fit in memory without slowing down system performance. If you're training on the GPU, that means you want to pick the largest batch size that will let your data and model fit on the GPU without getting a Cuda out of memory error. There's no reason you *have* to do it this way. Just that experiments have shown that models tend to train the best when you use as large a batch size as possible, without crashing your system or GPU.\n",
    "\n",
    "**Note:** It's also worth mentioning that there is a rough, rule of thumb tradeoff between learning rate and batch size. Essentially, if you multiply the batch size by a factor `k`, you'll want to make sure to also multiply the learning rate by that same factor `k` to ensure good training. For example, if you double the batch size, you should also double the learning rate, otherwise you'll be training with a smaller learning rate than you have to. If you halve the batch size, you should halve the learning rate, otherwise your model may fail to converge. This is just a rule of thumb though. Other rules of thumb have been proposed as well, but this linear rule is the easiest to remember. But at the end of the day, don't forget you always have the learning rate finder to help you out.\n",
    "\n",
    "In deep learning, when working with batches, a complete cycle through all of the data is called an **epoch**. When the training dataloader has exhausted all the batches and starts over, the model is said to have completed one epoch of training. Note the subtle difference between an *epoch* and an *iteration*. An iteration is how many times the model goes through the loop, not how many times it goes through the dataset. Before now this distinction didn't matter, since each iteration was also an epoch, as we were passing through all the data at once.\n",
    "\n",
    "Since we've covered dataloaders, we've covered the last *major* piece we need to add to our `train_model` function. Instead of taking in data in `X_train, y_train, X_test, y_test` format, we'll want to modify the function to take in dataloaders in `train_loader`, `test_loader` format. This also means we'll need to modify the training loop to iterate over these dataloaders separately in batches, first training and then inference. That means a double loop. Can't be avoided unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "589c2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, opt, loss_fn, num_epochs, test_loader=None, scheduler=None):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # training\n",
    "        model = model.train()\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat = model(X)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        avg_train_loss = loss / len(train_loader)\n",
    "        # inference\n",
    "        if test_loader is not None:\n",
    "            model = model.eval()\n",
    "            for X, y in test_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                yhat = model(X)\n",
    "                test_loss = loss_fn(yhat, y)\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "        else:\n",
    "            avg_test_loss = None\n",
    "        if num_epochs <= 10 or epoch % (num_epochs // 10) == 0:\n",
    "            print(f'epoch = {epoch} \\t\\t train loss = {avg_train_loss} \\t\\t test loss = {avg_test_loss}')\n",
    "    if not num_epochs <= 10 and not epoch % (num_epochs // 10) == 0:\n",
    "        print(f'epoch = {epoch} \\t\\t train loss = {avg_train_loss} \\t\\t test loss = {avg_test_loss}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e060854",
   "metadata": {},
   "source": [
    "Just to see how things look, let's create and train a light softmax regression model (i.e. linear regression with a softmax activation) on MNIST really quick. Recall that when doing multiclass classification with the cross entropy loss, we leave off the softmax from the model and just feed in the logits directly. This effectively means that our model is using just a single linear layer.\n",
    "\n",
    "We'll use a batch size of 128 (I could possibly go larger than that, but I didn't try). Let's train for 10 epochs and then check the test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd865eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = 28 * 28\n",
    "num_targets = 10\n",
    "\n",
    "model = nn.Linear(num_features, num_targets)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeec759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75000946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d6811474594f2896c0363fe3df7fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 5.34E-01\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAArVUlEQVR4nO3deXxV9bnv8c+zk5BAEgJhEggQFEQEkUkUcUJFLdbhOFQ99ZzaVmltnY491mNP69DT3vZcW1ttb23trVdrW4diB0Sseto6DxgGUaAKCkIYQwbCzpzs5/6xd2KIARLIyp6+79drv7L32r+91vPLhvXkN6zfMndHRETSVyjeAYiISHwpEYiIpDklAhGRNKdEICKS5pQIRETSnBKBiEiay4x3AN01ePBgLy4ujncYIiJJZdmyZbvcfUhn7yVdIiguLqakpCTeYYiIJBUz+2hf76lrSEQkzSkRiIikOSUCEZE0l3RjBJ1pamqitLSU+vr6eIciAcvJyaGoqIisrKx4hyKSMlIiEZSWlpKfn09xcTFmFu9wJCDuTnl5OaWlpYwdOzbe4YikjJToGqqvr2fQoEFKAinOzBg0aJBafiI9LCUSAaAkkCb0PUu6en7NDtbvDAey75RJBN3iDm+8AX/8Y/RnQPdk+PGPf0xtbW0g++6qqqoqfvazn/Xa8YqLi9m1axcAJ5544kHv56GHHmLr1q09FZZIUnN3vvLbZSxcVhrI/tMvESxZAqNHw7x5cNVV0Z+jR0e397BUSQTNzc0H9bnXXnvtoI+pRCDyser6ZppanMF5fQLZf3olgiVL4JJLoLQUwmGoro7+LC2Nbj/IZFBTU8O5557Lsccey+TJk3n88ce577772Lp1K3PnzmXu3LkAPPfcc8yePZvp06dz6aWXEg5Hm3nLli3j1FNPZcaMGZx99tls27YNgNNOO40bb7yRqVOnMnnyZJYuXdp2vC984QvMmjWLadOm8ec//xmA1atXM2vWLKZOncqUKVNYt24d//Ef/8EHH3zA1KlTueWWWz4R+3/9138xYcIETjrpJK644gp+8IMftB37pptuYubMmdx777089dRTHH/88UybNo0zzzyTHTt2AFBeXs5ZZ53FpEmTuPrqq2l/x7u8vLy253fffTfHHXccU6ZM4Y477gBg48aNTJw4kWuuuYZJkyZx1llnUVdXx8KFCykpKeGzn/0sU6dOpa6u7qC+F5FUUVHTCEBhbjCJAHdPqseMGTO8ozVr1nxi2ydEIu4jR7pHO4I6fxQVRct108KFC/3qq69ue11VVeXu7mPGjPGysjJ3dy8rK/OTTz7Zw+Gwu7t///vf97vuussbGxt99uzZvnPnTnd3f+yxx/zzn/+8u7ufeuqpbft98cUXfdKkSe7uftttt/kjjzzi7u6VlZU+fvx4D4fDft111/lvfvMbd3dvaGjw2tpa37BhQ9vnOlq6dKkfe+yxXldX59XV1T5u3Di/++6724597bXXtpWtqKjwSOx388tf/tJvvvlmd3e//vrr/a677nJ398WLFzvQVufc3Fx3d3/22Wf9mmuu8Ugk4i0tLX7uuef6iy++6Bs2bPCMjAxfsWKFu7tfeumlbfU69dRT/a233uo07i593yIp5K0N5T7m1sX+wns7D3ofQInv47yaEtNHu+TNN2H37v2XqaqCpUvh+OO7tetjjjmGr33ta9x66618+tOf5uSTT/5EmTfeeIM1a9YwZ84cABobG5k9ezbvvfce7777LvPmzQOgpaWF4cOHt33uiiuuAOCUU06hurqaqqoqnnvuORYtWtT213t9fT2bNm1i9uzZfPe736W0tJSLLrqI8ePH7zfuV199lQsuuICcnBxycnI477zz9nr/sssua3teWlrKZZddxrZt22hsbGybvvnSSy/xhz/8AYBzzz2XgQMHfuI4zz33HM899xzTpk0DIBwOs27dOkaPHs3YsWOZOnUqADNmzGDjxo37jVkkHe0KR1sEgwJqEaRPIti2DUIH6AkLheAg+qWPPPJIli9fzpIlS/jmN7/JGWecwe23375XGXdn3rx5PProo3ttf+edd5g0aRKvv/56p/vuOEvGzHB3nnzySSZMmLDXexMnTuT444/n6aefZv78+fziF7/g8MMP73Z9WuXm5rY9v/7667n55ps5//zzeeGFF7jzzju7vB9357bbbuNLX/rSXts3btxIdnZ22+uMjAx1A4l0orVraJDGCA7R8OEQiey/TCQCI0Z0e9dbt26lX79+XHnlldxyyy0sX74cgPz8fPbs2QPACSecwKuvvsr69euBaD//+++/z4QJEygrK2tLBE1NTaxevbpt348//jgAr7zyCgUFBRQUFHD22Wfzk5/8pK0/fsWKFQB8+OGHHH744dxwww1ccMEFrFq1aq8YOpozZw5PPfUU9fX1hMNhFi9evM867t69m5EjRwLw8MMPt20/5ZRT+N3vfgfAM888Q2Vl5Sc+e/bZZ/Pggw+2jYls2bKFnTt37vd3ur+4RdJNebgBCG6MIH1aBMcfDwUF0cHhfRkwAGbN6vau33nnHW655RZCoRBZWVncf//9ACxYsIBzzjmHESNG8Pe//52HHnqIK664goaG6Jf6ne98hyOPPJKFCxdyww03sHv3bpqbm7npppuYNGkSEF1SYdq0aTQ1NfHggw8C8K1vfYubbrqJKVOmEIlEGDt2LIsXL+aJJ57gkUceISsri8MOO4xvfOMbFBYWMmfOHCZPnsynPvUp7r777ra4jzvuOM4//3ymTJnCsGHDOOaYYygoKOi0jnfeeSeXXnopAwcO5PTTT2fDhg0A3HHHHVxxxRVMmjSJE088kdGjR3/is2eddRZr165l9uzZQHQQ+Te/+Q0ZGRn7/J1eddVVfPnLX6Zv3768/vrr9O3bt7tfi0jKKK9pJD87k+zMff+fOST7GjxI1MdBDxa7uz/9tHvfvp0PFPftG30/gexvwLSn7Nmzx93da2pqfMaMGb5s2bJAj9cTNFgs6ea63y33U//33w5pH+xnsDh9uoYA5s+HhQuhqAjy8qB//+jPoqLo9vnz4x1hr1uwYAFTp05l+vTpXHzxxUyfPj3eIYlIBxU1DcFNHSWduoZazZ8PmzZFZwdt3RodE5g1CxJw6YIXXngh8GO09u+LSOIqDzcyqrBfYPtPv0QA0ZN+N6eIiojES3lNI1NHDQhs/ynTNeQBrRckiUXfs6SbSMSpqGkMbOoopEgiyMnJoby8XCeJFOex+xHk5OTEOxSRXlNd30RLxCnMzT5w4YOUEl1DRUVFlJaWUlZWFu9QJGCtdygTSRetVxUHteAcpEgiyMrK0h2rRCQlBX0xGaRI15CISKpqW14iwK6hwBOBmWWY2Qoz+8T6BWaWbWaPm9l6M3vTzIqDjkdEJJnsqgm+a6g3WgQ3Amv38d4XgUp3Hwf8CPjvXohHRCRpVMTGCAYma9eQmRUB5wL/dx9FLgBaVzBbCJxhuimtiEib8poGCvpmkZUR3Ok66BbBj4GvA/ta9nMksBnA3ZuB3cCgjoXMbIGZlZhZiWYGiUg6Ka9pDOw+BK0CSwRm9mlgp7svO9R9ufsD7j7T3WcOGTKkB6ITEUkO5eGGQC8mg2BbBHOA881sI/AYcLqZ/aZDmS3AKAAzywQKgPIAYxIRSSoVNY2BTh2FABOBu9/m7kXuXgxcDvzN3a/sUGwR8LnY80tiZXR5sIhITHm4kUF5wU0dhThcUGZm3ya6LvYi4FfAI2a2HqggmjBERARoiTiVtcGPEfRKInD3F4AXYs9vb7e9Hri0N2IQEUk2VbWNRDy4m9a30pXFIiIJqvWq4sKAu4aUCEREElTbgnNqEYiIpKfymuiCc0EPFisRiIgkqLauIbUIRETS065wI2YwsF9WoMdRIhARSVAVNQ0M6JtFZoDrDIESgYhIwuqNi8lAiUBEJGGV98LyEqBEICKSsMrDDYHekKaVEoGISILqjQXnQIlARCQhNbdEqKxtCvRexa2UCEREElBlbRNA4PciACUCEZGE1HZVsVoEIiLpqTy2zpBaBCIiaao8trxE0EtQgxKBiEhCKg/3zoJzoEQgIpKQKmoaCRkM6BvsOkOgRCAikpB2VNdTmJtNKGSBH0uJQEQkAb23fQ9HDsvrlWMpEYiIJJjmlgj/2L6Ho4f375XjKRGIiCSYjeU1NDRHmKhEICKSntZs2wOgRCAikq7WbqsmK8MYN1RjBCIiaWnttmqOGJJHn8zeOUUrEYiIJJi126p7baAYlAhERBJKebiBHdUNHD0iBRKBmeWY2VIze9vMVpvZXZ2UucrMysxsZexxdVDxiIgkg7W9PFAMkBngvhuA0909bGZZwCtm9oy7v9Gh3OPufl2AcYiIJI2126qBFEkE7u5AOPYyK/bwoI4nIpIK1m6rZlj/7F65RWWrQMcIzCzDzFYCO4Hn3f3NTopdbGarzGyhmY0KMh4RkUS3Zlt1r7YGIOBE4O4t7j4VKAJmmdnkDkWeAordfQrwPPBwZ/sxswVmVmJmJWVlZUGGLCISNw3NLazfGU6tRNDK3auAvwPndNhe7u4NsZf/F5ixj88/4O4z3X3mkCFDAo1VRCRe1u8M0xzxXp06CsHOGhpiZgNiz/sC84B/dCgzvN3L84G1QcUjIpLo4jFjCIKdNTQceNjMMogmnCfcfbGZfRsocfdFwA1mdj7QDFQAVwUYj4hIQlu7rZqcrBBjB+f26nGDnDW0CpjWyfbb2z2/DbgtqBhERJLJ2m3VTBiWT0Yv3IymPV1ZLCKSANw9LjOGQIlARCQhbK+up6q2SYlARCRdrdsRvf52wmH5vX5sJQIRkQSwYVcNAIf38kAxKBGIiCSEDbtqyO2TwZD87F4/thKBiEgC2Fhew5hBuZj17owhUCIQEUkIG3fV9Pr1A62UCERE4qypJcLmyjqKB/eLy/GVCERE4qy0so6WiFM8SC0CEZG0tDE2Y0hdQyIiaap16mixEoGISHraWF5DfnYmg3rxrmTtKRGIiMTZhl01FA+Oz9RRUCIQEYm7DXGcOgpKBCIicdXQ3MLWqrq4jQ+AEoGISFxtrqgl4jA2TtcQgBKBiEhcbdhVCxC3awhAiUBEJK7ifQ0BKBGIiMTVhvIaBvTLYkC/+EwdBSUCEZG42rirJq7dQqBEICISV/GeOgpKBCIicVPX2MK23fVqEYiIpKuPKlrXGIrf1FFQIhARiZtEmDEESgQiInHTdg2BEoGISHrauKuGwXl96J+TFdc4lAhEROJkQ3n8p45CgInAzHLMbKmZvW1mq83srk7KZJvZ42a23szeNLPioOIREUk0H5XXMHpQfAeKIdgWQQNwursfC0wFzjGzEzqU+SJQ6e7jgB8B/x1gPCIiCaO+qYUd1Q2MKUzhFoFHhWMvs2IP71DsAuDh2POFwBkWrzsziIj0otLKOgBGFfaNcyRdTARmlmtmodjzI83sfDM74OiGmWWY2UpgJ/C8u7/ZochIYDOAuzcDu4FBnexngZmVmFlJWVlZV0IWEUlomyujM4ZGFyZP19BLQI6ZjQSeA/4FeOhAH3L3FnefChQBs8xs8sEE6e4PuPtMd585ZMiQg9mFiEhC2VyRfInA3L0WuAj4mbtfCkzq6kHcvQr4O3BOh7e2AKMAzCwTKADKu7pfEZFktam8luzMEEPys+MdStcTgZnNBj4LPB3blnGADwwxswGx532BecA/OhRbBHwu9vwS4G/u3nEcQUQk5WyurGVUYb+43bC+vcwulrsJuA34o7uvNrPDif6Fvz/DgYfNLINownnC3Reb2beBEndfBPwKeMTM1gMVwOUHUwkRkWSzqaIuIbqFoIuJwN1fBF4EiA0a73L3Gw7wmVXAtE62397ueT1waXcCFhFJdu5OaUUts4oHxjsUoOuzhn5nZv3NLBd4F1hjZrcEG5qISGqqqm1iT0MzoxKkRdDVMYKj3b0auBB4BhhLdOaQiIh006bYjKFkSwRZsesGLgQWuXsTn7w4TEREuiCRriGArieCXwAbgVzgJTMbA1QHFZSISCpLtBZBVweL7wPua7fpIzObG0xIIiKpbXNFHYW5fcjL7urEzWB1dbC4wMzuaV3mwcx+SLR1ICIi3bS5ojZhWgPQ9a6hB4E9wGdij2rg/wUVlIhIKttUUZsw4wPQ9QvKjnD3i9u9viu2mJyIiHRDc0uErVV1fHrK8HiH0qarLYI6Mzup9YWZzQHqgglJRCR1bdtdT3PEk7JF8GXg12ZWEHtdycdrBImISBe1Th1NpDGCrs4aehs41sz6x15Xm9lNwKoAYxMRSTmJtPx0q27doczdq2NXGAPcHEA8IiIpbVNFLRkhY3hBTrxDaXMot6qM/9qpIiJJZnNFHSMG5JCZEeQt47vnUCLREhMiIt2UaFNH4QBjBGa2h85P+AbE/47LIiJJprSylnlHD4t3GHvZbyJw9/zeCkREJNXVNDSzK9xI0cDEahEkTieViEiKS7RVR1spEYiI9JLNFdHrcBPpGgJQIhAR6TVty08PTKwhViUCEZFe8mFZmIK+WRTm9ol3KHtRIhAR6SUflIU5YkguZol1GZYSgYhIL/mgrIYjhuTFO4xPUCIQEekFu+uaKNvTwLihSgQiImnpg7IwgFoEIiLp6oOdsUSgFoGISHr6oKyGrAxLuKmjEGAiMLNRZvZ3M1tjZqvN7MZOypxmZrvNbGXscXtQ8YiIxNP6nWGKB+Um1Kqjrbp6h7KD0Qx8zd2Xm1k+sMzMnnf3NR3Kvezunw4wDhGRuPuwLMyEwxJz+bbAUpO7b3P35bHne4C1wMigjicikqgamyN8VFGbkAPF0EtjBGZWDEwD3uzk7dlm9raZPWNmk3ojHhGR3rSpooaWiHPE0Nx4h9KpILuGADCzPOBJ4KZ2t7lstRwY4+5hM5sP/AkY38k+FgALAEaPHh1swCIiPWz9zhogMaeOQsAtAjPLIpoEfuvuf+j4fuweyOHY8yVAlpkN7qTcA+4+091nDhkyJMiQRUR6XOs1BIenWyKw6GIavwLWuvs9+yhzWKwcZjYrFk95UDGJiMTDBzvDDC/IIS878E6YgxJkVHOAfwHeMbOVsW3fAEYDuPvPgUuAa82sGagDLnd33QtZRFJKdLG5xGwNQICJwN1fIXpv4/2V+Snw06BiEBGJN3fng7IaLp6euJMmE+/KBhGRFLKjuoFwQ3NCLi3RSolARCRArQPF4xK4a0iJQEQkQG2rjqpFICKSnj7YGSYvO5Oh+dnxDmWflAhERAK0PkFvT9meEoGISIA+2JmYt6dsT4lARCQge+qb2F5dn9DjA6BEICISmHdKdwNw9Ij+cY5k/xLzeucANDS3UF3XTMSdlogTccfM6JMRok9miOzMEH0yQoRCPduP13qhdGv/oLvT1OI0RyI0R5yczAz6ZCofi6Si5ZsqAZg+amCcI9m/tEkEz6/ZwXW/W3HAcn0yokkhKzOEu+OAO2SGjOzMENlZGfSJ3WEo+i60RJyG5kj00dRCU0s02TRFInRlwYw+GSFyszPIycpoS1LNkegHQ2aELJpIskJGVmaIrIxo0srKDNEnw6KvMz/elhUyIh6NqyXihEKQk5lBTp8McjIzYvujbf8WO0ZGyGiOOA1NERqaW2hsjmCx7WZGZsjIDIXIzIg+zwgZIYv+zGj3+8nODJFhFj2+x5IuRkaI2LE+/nxmyDCj7ffU+vuOxH73IaOtftmt9Yw9MkOh6O8q9vsG2uLNDBk5WRnkZWfSLzuD7MyM7vxzEekRyzdVccSQXAr6ZcU7lP1Km0QweUQB/3XBJEIhI8OMUMhwdxqbIzS2RH82NLfETuYRGltaMD4+CTdHIrETZLQcgBE9iYXMyM4KkZ0ZPQlmZRiZGSEyYydKgNZ8YLDX+/VNLYQbWqhpaKa+qaXtpJoRam1BRE+KrSe8ppYITS3RxNPUEonFHSHc0ByrS4TmFo+dpKMnxZaIUx87udc3RYjEzrrtT7iR2Ek7KxSKndCjCWev40eclpZogmtuiZZPlpWh+mSEyMvJJD/26Ncnk5ysDHIyQ+TEkldrgunXJ4P8nCzyczLpn5PFwH59GJibRWFuHwb260N2ZiihZ4BIYnB3Vmyq5MyJw+IdygGlTSIoHpxL8eDEvClEMvNYV1tzW6uohYamaEsoFKKt1dA+oUQi0OJOSySa1Frt1UqJvY44bQmusbndoyWaCDNDobbEaUT3G4nFU9cUTbC1jS3sqW9mT31T28/axhZ21zWxs6mF+qboHwCt+65tamlrYXQmK8PIz8kiLzuTgf2yGJKfzeC8bIbkZzO6sB9jB+cydnAuhbl9lDDS2IZdNVTWNjF9TGJ3C0EaJQIJhplFu4oyICcrA0jsJnBXuHtb8qiub6KqtomKmkYqaxupqGkk3PBxUqmsbWJLVT1vl+6mPNxA+/yRl53JYQU5DC/I4bD+OQztn82QvGyG5Eefjynsx5D8bCWLFLViUxUA00crEYgkHTMjNzuT3NiJvKuaWyKUVtaxYVcNG3bVsKmilu2769leXc/7O8rYFW78REujb1YGYwb1Y8ygfowa2I+igX0ZVdiPMYNyGV3YTxMJktjyTZXkZ2cyPsGnjoISgUiPycwItXVBzu3k/UjEqaxtpCzcwI7qBj4qr2Hjrlo2ltfwQVkNL75fRn1TpK18yKBoYLSradzQvLbHhMPy6Z+T/C2vVLd8UxVTRw/o8ZmIQVAiEOkloZAxKC+bQXnZHHUYwN63XXV3ysINbK6oiyWJGj7cVcOHZTW8uaF8ryRRPKgfk0cWMKWogGmjB3LMyIJY15wkgnBDM+9tr2be3HHxDqVLlAhEEoSZMTQ/h6H5OczoMMAYiThbqupYt3MPa7ft4Z3S3azYVMXiVduA6KyoySP7M330QI4pKmDyyALGDspNir9GU9GqzVVEHKYlwUAxKBGIJIVQyBhV2I9Rhf04/aiPpyPuCjew/KNKlsUev37jIxqboy2H3D4ZzD5iEOcdO4IzJg5L2PvlpqJkuZCslf5liCSxwXnZnDXpMM6adBgATS0R1u8M8+6W3awq3c3za3bwP2t3kp0Z4vSjhvKpY4Yzd8IQ8jXGEKhkuZCslRKBSArJyggxcXh/Jg7vz6UzR3HX+ZMo+aiSxau2suSd7Tzz7nb6ZIQ4afxgzpw4jFOOHEzRwH7xDjulJNOFZK2UCERSWChkzBpbyKyxhdxx3iRWbKrkL+9u5y+rt/O3f+wE4PDBuZw8fjD/NL2IqaMGxDfgFLCxvDZpLiRrpUQgkiYyQsbM4kJmFhfyn+dO5IOyMC+9v4uX15XxeMlmHn79I6aOGsDn5xTzqcnDdQ3DQVr+UWx8IAkuJGulRCCShsyMcUPzGTc0ny+cNJZwQzNPLivl4dc2cuNjK/lu/lo+d2IxVx4/Jmn6uRNFMl1I1kopX0TIy87kcycW8z83n8pDnz+OCYflc/ez73Hi9//Kt59aw5aquniHmBRaIs5f1+7k+MMLk2rqrloEItImFDJOmzCU0yYMZc3Wan758of8+vWNPPLGRi6eXsRXThvH6EEaXN6Xl9eVsb26njvOOzreoXSLWgQi0qmjR/TnR5dN5cWvz+WfZ43mDyu2MPeHL3Dz4yv5oCwc7/AS0u+XlTKgXxanTxwa71C6RYlARPZr5IC+3HXBZF75+ly+MKeYZ97dzrx7XuTfHl/Jh0oIbapqG3l+9Q4unDoy6W6EFFgiMLNRZvZ3M1tjZqvN7MZOypiZ3Wdm681slZlNDyoeETk0Q/vn8J/nHs3Lt87lmpMP5y/vbufMe17k5idWsm23xhCeensrjS0RLplRFO9Qui3IFkEz8DV3Pxo4AfiqmXXsOPsUMD72WADcH2A8ItIDBudlc9v8ibx861y+eNJYFq/axuk/eJH7/rqO+qaWeIcXN79fVsrE4f2ZPLIg3qF0W2CJwN23ufvy2PM9wFpgZIdiFwC/9qg3gAFmNjyomESk5wzOy+Y/zz2av958KqdNGMI9z7/PGT98kafe3oonyz1Me8h72/ewqnQ3lyZhawB6aYzAzIqBacCbHd4aCWxu97qUTyYLEUlgowr7cf+VM3j0mhPo3zeL6x9dwUX3v9a28Fo6+H3JZjJDxgVTR8Q7lIMSeCIwszzgSeAmd68+yH0sMLMSMyspKyvr2QBFpEfMPmIQi68/if998RRKK+u46Gevcf2jKyjb0xDv0ALV1BLhTyu3cMbEoQzKy453OAcl0ERgZllEk8Bv3f0PnRTZAoxq97ootm0v7v6Au89095lDhgzp+LaIJIiMkPGZ40bxwr+fxg1njOfZ1ds5+8cv8dzq7fEOLTCvrNvFrnAjl8wYdeDCCSrIWUMG/ApY6+737KPYIuBfY7OHTgB2u/u2oGISkd6Rm53JzfOO5OnrT+Kw/jkseGQZty5cRbihOd6h9bg3PiynT0aIk8cPjncoBy3IK4vnAP8CvGNmK2PbvgGMBnD3nwNLgPnAeqAW+HyA8YhILxs/LJ8/fXUOP/6f97n/xQ94/cNy7r18KtOSaEG2A3lrYwXHFCX3rUIDSwTu/gqw38U2PDq14KtBxSAi8dcnM8TXzzmKuUcN5abHVnLJz1/n384cz7WnjSMjidbj6Ux9UwvvbNnNF04aG+9QDomuLBaRXnFccSFLbjyZc48Zzg+ee58rHngj6Reze3tzFU0tznFjCuMdyiFRIhCRXlPQN4t7L5/KPZ85ljXbqpl/78tJPZBcErv3wIwkuglNZ5QIRKRXmRkXTS9i8fUnMaqwLwseWcadi1bT0Jx8VyW/tbGC8UPzGJjbJ96hHBIlAhGJi+LBuTx57Yl8fk4xD722kYt+9lpSLWLXEnGWfVTJzOLk7hYCJQIRiaPszAzuOG8Sv/zXmWypquPTP3mF35dsToolKt7fsYc99c0cV5zc3UKgRCAiCWDe0cN45saTmVJUwC0LV3HjYyuprm+Kd1j7VbKxAogOgic7JQIRSQjDC/ry26tP4N/POpKn39nGhT99NaG7it7aWMmw/tkUDewb71AOmRKBiCSMjJBx3enjefSaE6iqa+LC//Mqr6zbFe+wOlWysYKZxYVEF1FIbkoEIpJwZo0t5M9fncPwgr587v8t5ZHXN8Y7pL1sqapj6+56jkvyaaOtlAhEJCGNKuzHk185kdOOHMK3/ryaf3t8JXsSZNygdXwgFWYMgRKBiCSwvOxMHvjXmfzbmUey6O2tzL/vZZZ91O4+B+7wxhvwxz9Gf/bSbKO3NlaQl53JUYfl98rxgqZEICIJLSNk3HjmeJ740gm4w2d+8To//ds6/OmnYfRomDcPrroq+nP0aFiyJPCYSjZWMm30ADIzUuMUmhq1EJGUN2NMdK2i+ccMp+T+39J00cVQWgrhMFRXR3+WlsIllwSaDHaFG/jH9j0cPzY1uoUg2GWoRUR6VP+cLO677FjCN3yKPo37uPNZXR186UuwaRMEMKPn1fXRWUwnj0+dm2SpRSAiScWWLiW/vmb/haqqYOnSQI7/8rpdFPTNYvLIgkD2Hw9KBCKSXLZtg9ABTl2hEGzd2uOHdndeXlfGSeMGJ/29FNpTIhCR5DJ8OEQi+y8TicCIET1+6HU7w+yobkjq21J2RolARJLL8cdDwf67ZXzAAJg1q8cP/dL7ZQCcpEQgIhJHZvDAA9C38zV+6jL78Ksrb6WxpeevKXh53S4OH5JL0cB+Pb7veFIiEJHkM38+LFwIRUWQlwf9+0NeHl5UxLN3/pTv+Fj+5VdvUlXb2GOHrG9q4c0N5ZySQrOFWmn6qIgkp/nzo1NEly6NDgyPGIHNmsWFZrBiC19fuIp/+tlrPHjVcYwdnHvIh1v+USX1TZGUGx8AJQIRSWZm0TGDDi6cNpKRA/vypUeWcd5PXuH2847m0hlFh7RS6EvrdpGVYZxw+KBDiTghqWtIRFLSccWFLLpuDpNG9OfrC1dxza9L2Lmn/qD39/K6MqaPHkhudur9/axEICIpq2hgPx695gS+ee5EXlq3i7N/9BLPr9nR7f3sCjewems1pxyZeuMDoEQgIikuFDKuPvlwltxwEiMG9OWaX5fwvSVraWo5wLUI7Xy8rETqjQ+AEoGIpIlxQ/N58toTufKE0fzipQ/551++wfbdB+4qqm9q4Zcvf8jgvD5MGpE6y0q0p0QgImkjJyuD71x4DPdePpXVW6uZf9/LPLt6+34/c9dTq3l3SzXfu2hKSi0r0V5gicDMHjSznWb27j7eP83MdpvZytjj9qBiERFp74KpI1l03UkML8jhS48s4+sL3+707mdPlGzm0aWb+cppRzDv6GFxiLR3BNkieAg45wBlXnb3qbHHtwOMRURkL+OG5vHHr8zhq3OPYOGyUj5178s8uayUzRW1uDurt+7mW396lznjBvG1sybEO9xABTYPyt1fMrPioPYvInKo+mSGuOXsozj9qKHc/MTbfO33bwMwrH82LRFnYL8+3Hv5tJTtEmoV7wmxs83sbWAr8O/uvrqzQma2AFgAMHr06F4MT0TSwYwxhfz15lN5b8celn1UybKPKlm3I8x3/2kyg/Oy4x1e4MwDvNlzrEWw2N0nd/JefyDi7mEzmw/c6+7jD7TPmTNneklJSc8HKyKSwsxsmbvP7Oy9uM0acvdqdw/Hni8BsswsNSfpiogksLglAjM7zGILf5jZrFgs5fGKR0QkXQU2RmBmjwKnAYPNrBS4A8gCcPefA5cA15pZM1AHXO5B9lOJiEingpw1dMUB3v8p8NOgji8iIl2jK4tFRNKcEoGISJpTIhARSXNKBCIiaS7QC8qCYGZlwEf7KVIA7O7me51t77it/et9PR8M7NpPbF21vzp0p1xX69ud16rvoQuyvt39t9zxdevz3q7rgcqm2v/dA5UNor5j3L3zO+u4e0o9gAe6+15n2ztua/96P89Lgq5Dd8p1tb7dea36JnZ9u/tveV/17e26Hqhsqv3fjUd99/dIxa6hpw7ivc62d9z2VBee95Su7vNA5bpa3+68Vn0PXZD17e6/5Y6ve7q+3dlfOv3fPVDZIOq7T0nXNZTIzKzE97GWRypSfVNXOtUV0q++HaViiyCeHoh3AL1M9U1d6VRXSL/67kUtAhGRNKcWgYhImlMiEBFJc0oEIiJpTomgF5lZrpmVmNmn4x1LkMxsopn93MwWmtm18Y4naGZ2oZn90sweN7Oz4h1P0MzscDP7lZktjHcsQYn9X3049r1+Nt7xBE2JoAvM7EEz22lm73bYfo6ZvWdm683sP7qwq1uBJ4KJsmf0RF3dfa27fxn4DDAnyHgPVQ/V90/ufg3wZeCyIOM9VD1U3w/d/YvBRtrzuln3i4CFse/1/F4Ptpdp1lAXmNkpQBj4tcfuv2xmGcD7wDygFHgLuALIAL7XYRdfAI4FBgE5wC53X9w70XdPT9TV3Xea2fnAtcAj7v673oq/u3qqvrHP/RD4rbsv76Xwu62H67vQ3S/prdgPVTfrfgHwjLuvNLPfufs/xynsXhHYjWlSibu/ZGbFHTbPAta7+4cAZvYYcIG7fw/4RNePmZ0G5AJHA3VmtsTdI0HGfTB6oq6x/SwCFpnZ00DCJoIe+m4N+D7RE0fCJgHoue83GXWn7kSTQhGwkjToOVEiOHgjgc3tXpcCx++rsLv/J4CZXUW0RZBwSWA/ulXXWNK7CMgGlgQZWEC6VV/geuBMoMDMxnn0VqzJpLvf7yDgu8A0M7stljCS1b7qfh/wUzM7l2CWokgoSgS9zN0fincMQXP3F4AX4hxGr3H3+4ieONKCu5cTHQ9JWe5eA3w+3nH0lpRv8gRoCzCq3eui2LZUlE51BdU31evbXjrXvY0SwcF7CxhvZmPNrA9wObAozjEFJZ3qCqpvqte3vXSuexslgi4ws0eB14EJZlZqZl9092bgOuBZYC3whLuvjmecPSGd6gqqb6rXt710rvuBaPqoiEiaU4tARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMikJRhZuFePt5rvXy8AWb2ld48pqQHJQKRfTCz/a7F5e4n9vIxBwBKBNLjlAgkpZnZEWb2FzNbZmYvm9lRse3nmdmbZrbCzP7HzIbFtt9pZo+Y2avAI7HXD5rZC2b2oZnd0G7f4djP02LvLzSzf5jZb2NLU2Nm82PblpnZfWb2iftQmNlVZrbIzP4G/NXM8szsr2a23MzeMbMLYkW/DxxhZivN7O7YZ28xs7fMbJWZ3RXk71JSmLvroUdKPIBwJ9v+CoyPPT8e+Fvs+UA+vrL+auCHsed3AsuAvu1ev0Z0Se3BQDmQ1f54wGnAbqILloWILmNwEtGbEG0GxsbKPQos7iTGq4guf1wYe50J9I89HwysBwwoBt5t97mzgAdi74WAxcAp8f4e9Ei+h5ahlpRlZnnAicDvY3+gQ/SEDtGT9uNmNhzoA2xo99FF7l7X7vXT7t4ANJjZTmAY0RN3e0vdvTR23JVET9ph4EN3b933o8CCfYT7vLtXtIYO/K/YHbUiRNfMH9bJZ86KPVbEXucB44GX9nEMkU4pEUgqCwFV7j61k/d+Atzj7otiN9K5s917NR3KNrR73kLn/2+6UmZ/2h/zs8AQYIa7N5nZRqKti44M+J67/6KbxxLZi8YIJGW5ezWwwcwuhegtJc3s2NjbBXy87vznAgrhPeDwdrdH7OqN7QuAnbEkMBcYE9u+B8hvV+5Z4Auxlg9mNtLMhh562JJu1CKQVNLPzNp32dxD9K/r+83sm0AW8BjwNtEWwO/NrBL4GzC2p4Nx97rYdM+/mFkN0bXvu+K3wFNm9g5QAvwjtr9yM3vVzN4len/kW8xsIvB6rOsrDFwJ7Ozpukhq0zLUIgEyszx3D8dmEf0fYJ27/yjecYm0p64hkWBdExs8Xk20y0f9+ZJw1CIQEUlzahGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMiEBFJc/8fRUDLAjc+RFoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr_find(train_set, model, opt, loss_fn, batch_size=batch_size, start_lr=1e-5, end_lr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fbdf96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a2c917c88442feaceac36a17b34ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 \t\t train loss = 0.0007540841470472515 \t\t test loss = 0.005086470395326614\n",
      "epoch = 1 \t\t train loss = 0.00048541376600041986 \t\t test loss = 0.003749304451048374\n",
      "epoch = 2 \t\t train loss = 0.0007025026716291904 \t\t test loss = 0.004168649669736624\n",
      "epoch = 3 \t\t train loss = 0.0003640296054072678 \t\t test loss = 0.00046293591731227934\n",
      "epoch = 4 \t\t train loss = 0.0009896220872178674 \t\t test loss = 0.004241964314132929\n",
      "epoch = 5 \t\t train loss = 0.0006413895171135664 \t\t test loss = 0.00967457052320242\n",
      "epoch = 6 \t\t train loss = 0.0006371730705723166 \t\t test loss = 0.010192619636654854\n",
      "epoch = 7 \t\t train loss = 0.0005846598651260138 \t\t test loss = 0.0007608421146869659\n",
      "epoch = 8 \t\t train loss = 0.0005498883547261357 \t\t test loss = 0.001872978638857603\n",
      "epoch = 9 \t\t train loss = 0.0008496567024849355 \t\t test loss = 0.0036680528428405523\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = train_model(train_loader, model, opt, loss_fn, num_epochs, test_loader=test_loader, scheduler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ba845",
   "metadata": {},
   "source": [
    "Notice how quickly the model trained. It would be *much* slower if we tried to train it without using batches. Try it if you want. Take batch sizes of `len(train_set)` and `len(train_set)` and see how much slower training is.\n",
    "\n",
    "The loss barely changed over 10 epochs, but it does look pretty low on both ends. There's a hint the model is *underfitting* since the test loss appears so much lower than the training loss across much of training. That's not surprising, given we're just using softmax regression, which is logistic regression for multiple classes.\n",
    "\n",
    "Let's go ahead and write a function `get_scores` that can score our model using dataloaders with any sklearn score function `score_fn` we might pass in. Notice the loops look almost exactly the same as the training loop in the `train_model` function above. The main difference is that we're not calling `loss.backward()`, nor are we stepping the optimizer or the scheduler. This is what a standard inference loop looks like. Notice how we also set the model to eval mode at the top of the function.\n",
    "\n",
    "We'll use the function here to get the model accuracy on our training and test sets. It looks like we're already at about 92% test accuracy with this simple softmax regression model. Adding hidden layers will probably help up it even more, hopefully to get above the random forest accuracy of 96.8% from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de16645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg train accuracy_score: 0.9288268479033405\n",
      "avg test accuracy_score: 0.9215783227848101\n"
     ]
    }
   ],
   "source": [
    "def get_scores(train_loader, model, score_fn, test_loader=None):\n",
    "    # score_fn must be any valid sklearn scoring function that acts directly on labels\n",
    "    # currently only works for multiclass classification models, need to modify for binary or regression\n",
    "    model = model.eval()\n",
    "    train_scores = []\n",
    "    for X, y in train_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model(X).detach()\n",
    "        y_pred = yhat.argmax(dim=-1).long()\n",
    "        score = score_fn(y.flatten().cpu().numpy(), y_pred.flatten().cpu().numpy())\n",
    "        train_scores.append(score)\n",
    "    print(f'avg train {score_fn.__name__}: {sum(train_scores) / len(train_scores)}')\n",
    "    if test_loader is not None:\n",
    "        test_scores = []\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            yhat = model(X).detach()\n",
    "            y_pred = yhat.argmax(dim=-1).long()\n",
    "            score = score_fn(y.flatten().cpu().numpy(), y_pred.flatten().cpu().numpy())\n",
    "            test_scores.append(score)\n",
    "        print(f'avg test {score_fn.__name__}: {sum(test_scores) / len(test_scores)}')\n",
    "\n",
    "get_scores(train_loader, model, accuracy_score, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada127fd",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Changing from processing the entire dataset at once to just processing it in batches introduces new subtlety to training models that we didn't have before. When processing the full dataset, gradient descent works exactly one way. At each iteration the gradients were exact, meaning the model truly was moving in the \"best direction\" with each optimizer step, relative to the training dataset. But now we're moving to just training the model on batches. That means that each each iteration the gradients are calculated from that individual batch alone (a random sample of the training set), and hence they're no longer the exact \"true\" gradients from the full training set. \n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Since the gradients are used to decide which direction to step the update, and by how much to step, that means that we're now taking more noisy, random steps towards the minimum rather than marching straight in. So now our naive use of gradient descent, i.e. `SGD`, means something a bit different than it did before.\n",
    "\n",
    "This new version of gradient descent is called **stochastic gradient descent** or **SGD**, hence the reason for the pytorch name `SGD`. The inclusion of \"stochastic\" in the name refers to the fact that we're now using noisy gradients, which will tend to make our march to the minimum more noisy than before. That means a lower learning rate will be needed to not step \"too far\" off the mark, hence the longer the model will have to train to converge. It's kind of like the difference between walking home sober, and walking home a little drunk. When you're drunk (more noise), you'll tend to stagger around more and take longer tot get home than when you're sober (less noise).\n",
    "\n",
    "This extra randomness in gradient descent seems mostly undesired, since it means lower learning rates and longer training times. One way to control it is by tuning the batch size. The higher the batch size, the less randomness the optimizer will have, since it'll have a better estimate of its gradients and hence take better steps towards the minimum. A batch size of 1 is the *most random* version of SGD you can have, while a batch size of N is the *least random*.\n",
    "\n",
    "You've already seen it, but here again is an example of how to implement standard, vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce9fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d82d7",
   "metadata": {},
   "source": [
    "### SGD with Momentum\n",
    "\n",
    "Another way to tweak SGD to be less random is to use a concept called **momentum**. The idea is this: Our gradient estimates are noisy, so maybe we should smooth them out to make them less noisy. Instead of using the gradients directly to update the parameters, we'll use the smoothed gradients to update them. The smoothed gradients are calculated in a time series sense using *exponential smoothing*. This technique looks at all gradients calculated during training and takes an exponentially weighted average of them, so that the most recent gradients get higher weights than the least recent ones. The effect of this is kind of like a sliding window: to smooth the gradients out and make them less noisy.\n",
    "\n",
    "In more formal language SGD with momentum works as follows. Given a model with parameters $\\boldsymbol{\\theta}$, a learning rate $\\alpha$, and a **momentum** hyperparameter $\\mu$, each time step $t$ of the optimizer looks like\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\boldsymbol{m}_t & = \\mu \\boldsymbol{m}_{t-1} + \\frac{dL(\\boldsymbol{\\theta}_t)}{d\\boldsymbol{\\theta}}, \\\\\n",
    "    \\boldsymbol{\\theta}_t & = \\boldsymbol{\\theta}_{t-1} - \\alpha \\boldsymbol{m}_t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "The first line is the exponential smoothing, where we calculate the smoothed gradient $m_t$ that will then be fed into the gradient descent update in the second line. The momentum term $\\mu$ controls how much smoothing we want. Setting $\\mu=0$ means no smoothing, just regular SGD. Setting $\\mu$ infinitely high means infinite smoothing, i.e. the gradients $m_t$ won't update at all and the model won't learn. \n",
    "\n",
    "Note that by convention $m_0 \\equiv 0$, since it needs an initialization. This creates a slight bias towards zero in the gradient estimates early in training, but the effect dies out after a few iterations, so we can ignore it.\n",
    "\n",
    "In pytorch, using SGD with momentum just means adding an extra keyword `momentum` to SGD, which is the same as $\\mu$ in the equations above. For example, here's how to use SGD with momentum $\\mu=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7769e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e59c4",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "There were many other optimizers that came after SGD, e.g. RProp, RMSProp, and Adagrad. These SGD variants were popular for a few years, but they now all seem to be largely supplanted in deep learning. What ended up replacing them all was Adaptive Moment Estimation, or **Adam** for short. Adam was published in 2014, and even in 2022 it's not too out there to say that SGD and Adam are by far the most common optimizers used for deep learning, though newer improvements to Adam like RAdam and AdamW have gained some ground in the past few years.\n",
    "\n",
    "Adam works by not just using momentum to smooth the gradients, but to smooth their variances as well. The combination of smoothing both gradients and their variances has been shown to substantially speed up training even for the smallest batch sizes. The way Adam does the smoothing and updating is somewhat unusual though compared to what we've seen. I'll outline the steps, but don't worry if you don't understand what they're doing very well. \n",
    "\n",
    "Below is a more mathy explanation of how Adam works. If you're not inclined, you can feel free to skip to the bottom of this cell if you wish. Do at least pay attention to what the hyperparameters $\\beta_1, \\beta_2, \\varepsilon$ do at a high level though, should you ever care to tweak them during training (usually you won't).\n",
    "\n",
    "Instead of just keeping track of the smoothed gradients $m_t$, Adam also keeps track of the smoothed gradient variances $v_t$ (technically the gradient second moments). Annoyingly though, Adam calculates these smoothing terms in a slightly different way than SGD with momentum does,\n",
    "$$\\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + (1-\n",
    "\\beta_1) \\frac{dL(\\boldsymbol{\\theta}_t)}{d\\boldsymbol{\\theta}},$$\n",
    "$$\\boldsymbol{v}_t = \\beta_2 \\boldsymbol{v}_{t-1} + (1 - \\beta_2)\\bigg(\\frac{dL(\\boldsymbol{\\theta}_t)}{d\\boldsymbol{\\theta}}\\bigg)^2.$$\n",
    "\n",
    "Note the square of the gradient in the $\\boldsymbol{v}_t$ term is calculated *elementwise*. The values $\\beta_1, \\beta_2$ are the momentum hyperparameters, but in this formulation they only take values between 0 and 1. Values of $\\beta=0$ mean no smoothing, while values of $\\beta=1$ mean infinite smoothing. The original Adam paper recommends (and these are the pytorch defaults) beta values of $\\beta_1=0.9$ and $\\beta_2=0.999$. \n",
    "\n",
    "Notice that this is *a lot* of smoothing, especially on the variances. To get an idea how much smoothing it's doing, a good rule of thumb is that the number of previous iterations $n$ it's averaging over at any time $t$ is the last\n",
    "$$n \\approx \\frac{1}{1 - \\beta}$$\n",
    "iterations. So for $\\beta_1=0.9$, $m_t$ is averaging over about $n\\approx 10$ iterations. For $\\beta_2=0.999$, $v_t$ is averaging over about $n\\approx 1000$ iterations. Note these are iterations (the number of batches the model sees), not epochs. Each epoch can have thousands or millions of iterations. Another effect of this is that the biases die off pretty quickly, since $m_0=0$ will be forgotten in just 10 iterations, and $v_0=0$ in just 1000 iterations.\n",
    "\n",
    "Despite the fact that the biases die off quickly, Adam implements **bias correction** on these smoothed gradients to ensure that the $t=0$ terms have no impact early on. Bias correction is done by dividing each smoothed estimate by a factor $1/(1-\\beta)$, like so:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\boldsymbol{\\hat m}_t & = \\frac{\\boldsymbol{m}_t}{1 - \\beta_1^t}, \\\\\n",
    "    \\boldsymbol{\\hat v}_t & = \\frac{\\boldsymbol{v}_t}{1 - \\beta_2^t}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since we showed the bias terms $m_0,v_0$ die off pretty quickly in training, these corrections really don't do that much, as $\\beta^t \\approx 0$ when $t$ gets large, hence $\\boldsymbol{\\hat m}_t \\approx \\boldsymbol{m}_t$ and $\\boldsymbol{\\hat v}_t \\approx \\boldsymbol{v}_t$. Nevertheless, these terms are still included in the algorithm.\n",
    "\n",
    "The final step is to update the parameters. This will be done as follows.\n",
    "$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\alpha \\frac{\\boldsymbol{\\hat m}_t}{\\sqrt{\\boldsymbol{\\hat v}_t}+\\varepsilon}.$$\n",
    "We use the $\\boldsymbol{\\hat m}_t$ term here just like we did $\\boldsymbol{m}_t$ in SGD with momentum. The main difference is we're now (elementwise) dividing by the gradient \"standard deviations\" $\\sqrt{\\boldsymbol{\\hat v}_t}$. Since these values can become zero (why not) and make the denominator zero, we'll add a very small numerical value $\\varepsilon$ to keep that from happening. This ensures that even if $\\boldsymbol{\\hat v}_t \\approx 0$ that the denominator will never be zero, which is better for the optimizer's numerical stability. The default value for epsilon is $\\varepsilon=10^{-8}$, a really small number indeed.\n",
    "\n",
    "We can optionally add in **weight decay** to Adam just like with SGD. It turns out though that the authors implemented weight decay incorrectly, using the same weight decay factor as used with SGD. It'll still work mostly well in practice if you use it, but it's not *really* weight decaying correctly anymore, since the smoothing terms actually change what the gradients of the loss look like, and hence what the weight decay penalty term looks like. A newer optimizer called AdamW will rectify this little deficiency.\n",
    "\n",
    "Even still, Adam is still the most popular optimizer used to train deep learning models, and we'll probably end up using it most of the time in these tutorials. Adam is a pretty fast optimizer, allowing you to use learning rates much higher than SGD would typically let you get away with.\n",
    "\n",
    "In pytorch, we can implement Adam using `torch.optim.Adam`. Here's an example, where we're using the default values for the betas and epsilon. (You don't actually have to pass them in if they're default values. I just do it so you can see them if you ever want to tweak them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c5a2673",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d7b41",
   "metadata": {},
   "source": [
    "The next two optimizers we'll mention are newer. They're just minor tweaks to Adam designed to correct some of its deficiencies. They mostly still work exactly the same as Adam does. Nevertheless, they are becoming more widely used.\n",
    "\n",
    "### AdamW\n",
    "\n",
    "AdamW works exactly the same way Adam does, except it does one tweak at the beginning of each step to make weight decay come out correctly (recall that it doesn't come out quite correctly for Adam). \n",
    "\n",
    "The calculations for $\\boldsymbol{\\hat m}_t, \\boldsymbol{\\hat v}_t$ are exactly the same. The only difference is that we tweak the gradient update step at the end by subtracting the correction term shown in red below,\n",
    "$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\alpha \\frac{\\boldsymbol{\\hat m}_t}{\\sqrt{\\boldsymbol{\\hat v}_t}+\\varepsilon} \\color{red}{- \\alpha \\lambda \\boldsymbol{\\theta}_{t-1}}.$$\n",
    "\n",
    "This small correction ensures that weight decay for Adam works exactly like it's supposed to, i.e. that the L2 penalty term from the regularized loss function gives the correct weight decay penalty term in the gradient update.\n",
    "\n",
    "Notice if $\\lambda=0$, we just get the regular Adam update. That is, AdamW reverts to just plain Adam if no weight decay is used.\n",
    "\n",
    "Implementing AdamW in pytorch works exactly the way Adam does, even taking in the same parameters. Here's an example using the defaults, with a light weight decay added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3429af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210670b",
   "metadata": {},
   "source": [
    "### RAdam\n",
    "\n",
    "One strange feature of using Adam with some of the modern architectures we haven't covered yet, like transformers, is the need for *warmups* early in training. Essentially, with some types of neural network architectures, Adam has a tendency early in training to diverge or converge to poor minima. What seems to be happening is that the variance term $v_t$ is getting too small early in training, causing the *effective learning rate*\n",
    "$$\\alpha_{\\text{eff}} \\equiv \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon}$$\n",
    "to get really large, which as you'll recall means that training will fail to converge since the optimizer just bounces around the loss surface. This problem only seems to happen early in training, with $v_t$ eventually settling down to larger values later on, bringing the effective learning rate back down again. Nevertheless, this tendency to bounce around early in training can mean longer training times or having to tune the hyperparameters more.\n",
    "\n",
    "One way around this warmups issue is to use a learning rate scheduler that includes a warmup period. That is, instead of starting with the specified learning rate, it starts training with a much lower learning rate, and ramps up to the specified learning rate after so many iterations. One example of a scheduler that does this is one-cycle, which starts low and ramps up the learning rate to the max over a specified number of iterations (usually something like a third of an epoch). If you use Adam with a one-cycle scheduler, you'll probably be fine.\n",
    "\n",
    "Another way around this is to just go ahead and \"fix\" Adam to keep this early divergence issue from happening in the first place, essentially baking a warmup period into the optimizer directly. This is what Rectified Adam, or **RAdam**, does. It again essentially reproduces Adam except for some tweaking, the introduction of a *rectifier* term $\\rho_t$ that's designed to keep the effective learning rate from blowing up early in training.\n",
    "\n",
    "$$\\color{black}{\\rho_{\\infty} = \\frac{2}{1-\\beta_2}-1}$$\n",
    "$$\\color{black}{\\rho_t = \\rho_{\\infty} - 2t\\frac{\\beta_2^t}{1-\\beta_2^t}}$$\n",
    "$$\\color{black}{r_t = \\sqrt{\\frac{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2)\\rho_t}}}$$\n",
    "$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\alpha \\frac{\\boldsymbol{\\hat m}_t}{\\sqrt{\\boldsymbol{\\hat v}_t}+\\varepsilon} \\color{red}{r_t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12431bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_2 = 0.999\n",
      "rho_inf = 1998.9999999999982\n",
      "t               rho_t                     r_t                       rho_t > 5      \n",
      "------------------------------------------------------------------------------------\n",
      "1               1.0                       0.038797750152842335      0              \n",
      "10              9.98349177230557          0.04899801344710748       1              \n",
      "100             98.33294432272578         0.21533543653820142       1              \n",
      "1000            835.9673756107934         0.6453256523380656        1              \n",
      "10000           1998.0964922659898        0.9997733049343513        1              \n",
      "100000          1998.9999999999982        1.0                       1              \n"
     ]
    }
   ],
   "source": [
    "beta = 0.999\n",
    "rho_inf = 2/(1-beta)-1\n",
    "print(f'beta_2 = {beta}')\n",
    "print(f'rho_inf = {rho_inf}')\n",
    "print(f'{\"t\":<15} {\"rho_t\":<25} {\"r_t\":<25} {\"rho_t > 5\":<15}')\n",
    "print('-'*14*6)\n",
    "for t in [1, 10, 100, 1000, 10_000, 100_000]:\n",
    "    rho_t = rho_inf - 2*t*beta**t/(1-beta**t)\n",
    "    r_t = np.sqrt(((rho_t-4)*(rho_t-2)*rho_inf) / ((rho_inf-4)*(rho_inf-2)*rho_t))\n",
    "    print(f'{t:<15} {rho_t:<25} {r_t:<25} {rho_t > 5: <15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebedaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.RAdam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc90030e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{aligned}\n",
       "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
       "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\beta_1, \\beta_2\n",
       "            \\text{ (betas)}, \\: \\theta_0 \\text{ (params)}, \\:f(\\theta) \\text{ (objective)}, \\:\n",
       "            \\lambda \\text{ (weightdecay)},                                                   \\\\\n",
       "        &\\hspace{13mm} \\epsilon \\text{ (epsilon)}                                            \\\\\n",
       "        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
       "            v_0 \\leftarrow 0 \\text{ ( second moment)},                                       \\\\\n",
       "        &\\hspace{18mm} \\rho_{\\infty} \\leftarrow 2/(1-\\beta_2) -1                      \\\\\n",
       "        &\\rule{110mm}{0.4pt}  \\\\\n",
       "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
       "        &\\hspace{6mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
       "        &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n",
       "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n",
       "        &\\hspace{6mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
       "        &\\hspace{6mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
       "        &\\hspace{6mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
       "        &\\hspace{6mm}\\rho_t \\leftarrow \\rho_{\\infty} -\n",
       "            2 t \\beta^t_2 /\\big(1-\\beta_2^t \\big)                                    \\\\\n",
       "        &\\hspace{6mm}\\textbf{if} \\: \\rho_t > 5                                               \\\\\n",
       "        &\\hspace{12mm} l_t \\leftarrow \\sqrt{ (1-\\beta^t_2) / \\big( v_t +\\epsilon \\big) }     \\\\\n",
       "        &\\hspace{12mm} r_t \\leftarrow\n",
       "  \\sqrt{\\frac{(\\rho_t-4)(\\rho_t-2)\\rho_{\\infty}}{(\\rho_{\\infty}-4)(\\rho_{\\infty}-2) \\rho_t}} \\\\\n",
       "        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t} r_t l_t        \\\\\n",
       "        &\\hspace{6mm}\\textbf{else}                                                           \\\\\n",
       "        &\\hspace{12mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}                \\\\\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\\n",
       "        &\\bf{return} \\:  \\theta_t                                                     \\\\\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\\n",
       "   \\end{aligned}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-layer NN 784-2500-2000-1500-1000-500-10 (on GPU) [elastic distortions]\n",
    "# accuracy = 99.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590aa917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec2cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
