{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55be8c1c",
   "metadata": {},
   "source": [
    "# Neural Networks (Part 3): Big Data\n",
    "\n",
    "In part 3 of this neural networks series of tutorials we'll adress some techniques for dealing with larger datasets that are just too impractical to train on all in one go. We'll discuss:\n",
    "- Datasets and Dataloaders\n",
    "- Batching\n",
    "- Optimizers: SGD, Adam, RAdam, AdamW\n",
    "\n",
    "We'll again load the usual libraries we'll need, set a seed, and set a device for those who'd prefer to work on the GPU. We'll also continue to use our MLP creator function, our parameter counter, and our learning rate finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91afda63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c78a710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dccafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ebdae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542454d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_mlp(num_features, num_targets, hidden_sizes, p=0, normalization=None, activation=nn.ReLU()):\n",
    "    \"\"\"\n",
    "    Get an arbitrary MLP model with L=len(hidden_sizes)+1 layers\n",
    "    Optionally, can include a batchnorm and dropout in each layer, and change the activation function\n",
    "    \"\"\"\n",
    "    def get_leaf_layers(m):\n",
    "        \"\"\"Used to flatten the layers out so the model doesn't recursively nest\"\"\"\n",
    "        children = list(m.children())\n",
    "        if not children:\n",
    "            return [m]\n",
    "        leaves = []\n",
    "        for l in children:\n",
    "            leaves.extend(get_leaf_layers(l))\n",
    "        return leaves\n",
    "    hidden_sizes = [num_features] + hidden_sizes\n",
    "    n_layers = len(hidden_sizes)\n",
    "    model = nn.Sequential()\n",
    "    for l in range(n_layers-1):\n",
    "        linear = nn.Linear(hidden_sizes[l], hidden_sizes[l+1])\n",
    "        dropout = None if p == 0. else nn.Dropout(p)\n",
    "        layer_norm = None if normalization is None else normalization(hidden_sizes[l+1])\n",
    "        blocks = [linear, layer_norm, activation, dropout]\n",
    "        layer_block = nn.Sequential(*[layer for layer in blocks if layer is not None])\n",
    "        model = nn.Sequential(*([model] + [layer_block]))\n",
    "    output = nn.Sequential(nn.Linear(hidden_sizes[-1], num_targets))\n",
    "    model = nn.Sequential(*([model] + [output]))\n",
    "    model = nn.Sequential(*get_leaf_layers(model))\n",
    "    return model\n",
    "\n",
    "def lr_find(dataset, model, opt, loss_fn, batch_size=128, plot=True, log_lr=True, **kwargs):\n",
    "    from torch_lr_finder import LRFinder\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    lr_finder = LRFinder(model, opt, loss_fn, device=device)\n",
    "    lr_finder.range_test(dataloader, **kwargs)\n",
    "    lr_finder.plot(log_lr=log_lr)\n",
    "    lr_finder.reset()\n",
    "    \n",
    "def num_params(model):\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28efb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(X_train, y_train, model, opt, loss_fn, num_iters, X_test=None, y_test=None, scheduler=None):\n",
    "#     for i in tqdm(range(num_iters)):\n",
    "#         # training\n",
    "#         model = model.train()\n",
    "#         opt.zero_grad()\n",
    "#         yhat = model(X_train)\n",
    "#         loss = loss_fn(yhat, y_train)\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         if scheduler:\n",
    "#             scheduler.step()\n",
    "#         avg_train_loss = loss / len(X_train)\n",
    "#         # inference\n",
    "#         if X_test is not None and y_test is not None:\n",
    "#             model = model.eval()\n",
    "#             yhat = model(X_test)\n",
    "#             test_loss = loss_fn(yhat, y_test)\n",
    "#             avg_test_loss = test_loss / len(X_test)\n",
    "#         else:\n",
    "#             avg_test_loss = None\n",
    "#         if i % (num_iters // 10) == 0:\n",
    "#             print(f'iter = {i} \\t\\t train loss = {avg_train_loss} \\t\\t test loss = {avg_test_loss}')\n",
    "#     print(f'iter = {i} \\t\\t train loss = {avg_train_loss} \\t\\t test loss = {avg_test_loss}')\n",
    "#     return model\n",
    "\n",
    "# model = train_model(X_train, y_train, model, opt, loss_fn, num_iters, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da0346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fa55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46698ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ded98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80f553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3ad5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454418ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9e90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fceae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
