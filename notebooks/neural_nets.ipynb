{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cfdf041-4023-48ca-9887-833d04d546b2",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this tutorial, we'll move on from classical techniques and frameworks like sklearn, and start building deep learning models in pytorch. Since this tutorial is intended to be more of an introduction to deep learning for practitioners, I won't do much of a deep dive into the math and the theory. The focus will instead be on understanding what a neural network is, creating a basic neural network in pytorch, and how to train simple deep learning models. In later tutorials, we'll go into more specialized topics that touch on state of the art approaches, like convolutional neural networks for computer vision, and transformers for natural language processing.\n",
    "\n",
    "So what is deep learning? In essense, **deep learning** is just a fancy rebrand from 15 or so years ago for neural networks. At the time neural nets weren't very well respected in the AI community, mostly because they didn't perform very well on the smaller datasets and limited compute available at the time. The term was chosen partly because of the growing tendency to use *deep* neural networks, i.e. networks with lots of hidden layers. Prior to that time, most neural networks had just a few hidden layers, maybe 1 to 5 tops. Starting with the release of Cuda, as well as a few algorithmic innovations to make training neural networks easier, they started to get a lot deeper, with some neural networks today even containing thousands of hidden layers.\n",
    "\n",
    "So what is a neural network, then? Very roughly speaking, a **neural network** can be thought of as a fancy form of linear or logistic regression that contains a bunch of intermediate steps (called **layers**) designed to make linear or regression work better by transforming the problem to an easier space of features. These intermediate steps are thought of as \"learned features\". The term neural network itself comes from a loose analogy with the brain, which is sometimes believed to perform computations in a loosely similar way to the abstract neural nets we'll describe and use.\n",
    "\n",
    "Once the neural network has \"learned\" all of these learned features, it's basically just going to apply vanilla linear or logistic regression to those learned features to make predictions in the usual way. The power of deep learning comes from the fact that, with enough data and compute, a neural net can *learn* the best features to make predictions on, rather than humans having to manually feature engineer them by hand. This idea that a model can take in raw inputs, learn the relevant features on its own, and make predictions is called **end-to-end learning**. \n",
    "\n",
    "As deep learning gets better and better, tasks that were once heavily feature engineered are getting more end-to-end, the most notable example being image classification. It used to be that images had to be heavily feature engineered before doing classification on them. Computer vision researchers spent decades coming up with fancy hand engineered features, like edge filters and SIFT features, to do image classification, because they thought such a task was far too complicated to do from raw images alone. Since the rise of deep learning, however, neural networks trained on raw images are doing far better on image classification tasks than any of these feature engineered approaches had managed to do, and it's become widely accepted that end-to-end deep learning is the best way to go when trying to classify images. This same trend has since happened to speech recognition, machine translation, text classification, language modeling, image generation, and many other areas as the years go by.\n",
    "\n",
    "Before diving into more specifics of what neural networks are made of let's go ahead and import pytorch. The submodule of pytorch that contains everything relevant for deep learning is called `nn`, short for \"neural networks\". We'll import this submodule separately, along with its library of functions, usually just aliased to `F`. We'll also go ahead and set a device for those who'd prefer to work on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65853409-5a33-4cae-93bf-caabaacf351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1076b26f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29e7c677-1c96-4ca8-b597-549be06c6caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757931b",
   "metadata": {},
   "source": [
    "## Linear Regression as a Neural Network\n",
    "\n",
    "We already learned a bit about what linear regression is. Recall that linear regression tries to learn **parameters** $W,b$ to best fit each input-output pair $(x_i,y_i)$ in the dataset with a linear model defined by\n",
    "$$\\hat y_i = \\hat y_i(W,b|x_i) = Wx_i + b.$$\n",
    "It does this by minimizing the **mean squared error** loss function over all of the training data $i=1,\\cdots,N$:\n",
    "$$L(W,b) = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat y_i(W,b|x_i)\\big)^2.$$\n",
    "In deep learning lingo, this minimization procedure to find the best fit parameters $W,b$ is called **learning**, or **training** the model. The linear transformation $\\hat y = Wx + b$ is called a **linear layer** or a **dense layer** of the neural network.\n",
    "\n",
    "To get a linear layer in pytorch (with randomly initialized parameters) we just define an object `nn.Linear(num_features, num_targets)`, where `num_features` is the number of input features and `num_targets` is the number of target variables. We can pass the input data `X` through it just like we would any ordinary function.\n",
    "\n",
    "Let's look at a simple example, where `num_features = num_targets = 1`, the simple kind you learn about in intro statistics classes. We'll generate some linear data `X, y` with light gaussian noise and then try to model it with a linear layer. Notice that:\n",
    "- We can treat `model` just like any ordinary function that takes in data and outputs predictions.\n",
    "- The predictions `yhat` are automatically part of the computational graph to calculate gradients, meaning we have make sure to detach it before converting it to a numpy array.\n",
    "- The fit of the line is pretty poor. Not surprising, given we haven't actually *learned* anything yet. This model is just using randomly initialized values for `W` and `b`. They'll stay that way until we tune them with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2e8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1\n",
    "num_targets = 1\n",
    "num_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ebd11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.linspace(0, 5, num_samples).reshape(-1, 1)\n",
    "y = X + torch.randn(num_samples).reshape(-1, 1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb97e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(num_features, num_targets)\n",
    "yhat = model(X)\n",
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783d972c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy(), label='$y$', color='black')\n",
    "plt.plot(X.numpy(), yhat.detach().numpy(), label='$\\hat y$', color='red')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression - Untrained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a8250",
   "metadata": {},
   "source": [
    "To actually access the parameter values themselves, we can use the `parameters` method of the `model` object. This will always be true for accessing neural net parameters in pytorch. By default, `model.parameters()` gives a python **generator** object, which we need to first convert into a list to get `[W, b]`. An easier way for linear layers is to just use `model.weight` to get the weights `W`, and `model.bias` to get the bias `b`. Notice that each parameter has `requires_grad=True` on, which makes sense, given that those are the things we want to learn. This will always be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b44f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Parameter containing:\n",
       "  tensor([[-0.8457]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.2869], requires_grad=True)],\n",
       " Parameter containing:\n",
       " tensor([[-0.8457]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2869], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "W = model.weight\n",
    "b = model.bias\n",
    "params, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581a472f",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "To actually learn the parameters that give the best fit we'll use an algorithm called **gradient descent**, which is the basis for just about every learning algorithm in deep learning. Gradient descent is a method for solving minimization problems that involves using the derivative (i.e. the gradient) of a function to find its minimum as quickly as possible. More formally, given a loss function $L(\\theta)$ to minimize with respect to some parameters $\\theta$, gradient descent seeks to find the minimum $\\hat \\theta$ by iteratively computing the update\n",
    "$$\\theta_{n+1} = \\theta_n - \\alpha \\frac{dL(\\theta_n)}{d\\theta}$$\n",
    "until convergence, i.e. until the difference $\\Delta \\theta_n \\equiv \\theta_{n+1} - \\theta_n$ is \"small enough\". When doing deep learning in practice, rather than specify some convergence criterion like this, it's more typical to just run gradient descent for so many iterations and stop it when the loss doesn't seem to be decreasing much anymore.\n",
    "\n",
    "The value $\\alpha$ in the update is called the **learning rate**, or **step size**. It controls how big the update is at each step of the iteration, and is generally manually specified by the programmer before training starts. Since the learning rate controls the speed of learning, the goal is to make it as high as possible so the algorithm will converge quicker. Unfortunately though there are limits to this. If you make the learning rate too large the updates will spiral off to infinity or just oscillate forever and fail to converge. So there's an art to tuning the learning rate, by making it as big as you can but without making it too big.\n",
    "\n",
    "Let's apply gradient descent to our above linear regression problem to see if we can learn the best fit line to the given data. Before training our linear regression model we need to know what function it is we're trying to minimize. In deep learning this will always be the loss function. For regression tasks, the loss function is usually the MSE loss defined above:\n",
    "$$L(W,b) = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat y_i(W,b|x_i)\\big)^2.$$\n",
    "\n",
    "In pytorch, the loss is essentially just another layer that we can define and pass data through. To get the MSE loss we can just use the `nn.MSE` layer and then pass our predictions `yhat` through it to calculate the value. Note that in pytorch the order to pass arguments into the loss is always `yhat, y`, not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a375115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.4925, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(yhat, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277d381",
   "metadata": {},
   "source": [
    "Let's now try to minimize the loss function using gradient descent. Rather than code gradient descent from scratch, in pytorch it's better to define it as an **optimizer** object, and then step the optimizer until we're done training. The optimizer for gradient descent is called `torch.optim.SGD`, where SGD stands for **stochastic gradient descent**, which is a slight variation of gradient descent that works better for big datasets.\n",
    "\n",
    "To use the optimizer object, we have to pass in the **parameters** we want to minimize, not the model itself. It's also here that we pass in the learning rate `lr` that'll be used for training the model. We'll just use `lr=0.1` here. There are also other values we can pass in to improve the optimization, but we won't go into these right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5f76e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    foreach: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e2b2bd",
   "metadata": {},
   "source": [
    "Now everything is in place to begin training the model. What you're about to see is the standard backbone of training loops in pytorch. Some version of this loop is used to train the vast majority of deep learning models in pytorch.\n",
    "\n",
    "Even though this example is simple enough that we don't need to, it's common to visualize training by printing out the value of the loss every few iterations. It's also common to plot the loss curve either during training or at the end of training.\n",
    "\n",
    "We can see that the loss is indeed decreasing towards zero, which is exactly what we want here, since the MSE loss must always be a positive value (it's a sum of squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf96904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 \t\t loss = 28.492538452148438\n",
      "iter = 10 \t\t loss = 2.3468408584594727\n",
      "iter = 20 \t\t loss = 1.031928300857544\n",
      "iter = 30 \t\t loss = 0.9647798538208008\n",
      "iter = 40 \t\t loss = 0.9609823226928711\n",
      "iter = 50 \t\t loss = 0.9606364965438843\n",
      "iter = 60 \t\t loss = 0.9605627059936523\n",
      "iter = 70 \t\t loss = 0.9605387449264526\n",
      "iter = 80 \t\t loss = 0.9605301022529602\n",
      "iter = 90 \t\t loss = 0.9605270624160767\n"
     ]
    }
   ],
   "source": [
    "num_iters = 100\n",
    "losses = []\n",
    "for i in range(num_iters):\n",
    "    opt.zero_grad()\n",
    "    yhat = model(X)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 10 == 0:\n",
    "        print(f'iter = {i} \\t\\t loss = {loss}')\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e39a17",
   "metadata": {},
   "source": [
    "Let's go ahead and look at the parameters again and see if we found our best fit line to the data. It certainly looks like they've updated. The plot shows a much better fit to the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a01134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0.7272]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4537], requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = model.weight\n",
    "b = model.bias\n",
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b31ae8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = model(X)\n",
    "\n",
    "plt.scatter(X.numpy(), y.numpy(), label='$y$', color='black')\n",
    "plt.plot(X.numpy(), yhat.detach().numpy(), label='$\\hat y$', color='red')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression - Untrained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a2f49",
   "metadata": {},
   "source": [
    "Before moving onto logistic regression, it's worth showing how to actually implement a custom layer in pytorch. To do so we'll implement the above linear layer `nn.Linear` from scratch. In pytorch, a layer is any python class that\n",
    "1. inherets from the nn.Module class,\n",
    "2. implements a `forward` method that maps the layer input to the layer output.\n",
    "\n",
    "Since we want parameters `W, b` that update with gradient descent, we need to keep track of their gradients, which means they need to have autodiff turned on. To play nice with the `nn` framework, the best way to do this for parameters is to wrap them inside `nn.Parameter`. This tells the layer that these are intended to be learnable parameters, and to keep track of them inside `model.parameters()`.\n",
    "\n",
    "You always want to define your parameters inside the `__init__` method so that the layer object can keep track of them. The `forward` method is where the actual operation happens, which in this case is just the (vectorized) linear function\n",
    "$$\\hat y = WX + b.$$\n",
    "Note that for linear layers the $WX$ is a *matrix multiply*, not an elementwise product. The bias vector $b$ is generally added to it directly using the standard numpy broadcasting rules. \n",
    "\n",
    "**Note:** There's always a question in deep learning of how to **initialize** the parameters at the beginning of training. There are a lot of ways to do this, and a lot of reasons for each, but for linear regression we'll be okay just initializing the weights using `torch.rand` and the bias using `torch.zeros`.\n",
    "\n",
    "**Note:** We need to transpose the input and output vectors in `forward` due to the fact that we generally want to keep data in the format `(num_samples, num_features)` format. But the matrix multiply wants a `(num_features, num_samples)` format instead. An alternative that's often used is to not transpose the data, but to instead treat the feature vectors as row vectors and transpose the weight matrix $W$ instead:\n",
    "$$\\hat y = X W^T + b.$$\n",
    "\n",
    "Once you've implemented a custom layer like this, it's always a good idea to first check that it works by passing some data through the layer and making sure things look reasonable. In this case, it looks like we're okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e822e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(num_targets, num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_targets))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.T\n",
    "        yhat = self.weight @ X + self.bias\n",
    "        return yhat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acabb72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 1]), torch.Size([20, 1]), torch.Size([20, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear(num_features, num_targets)\n",
    "yhat = linear(X)\n",
    "X.shape, y.shape, yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea3ddb",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235026d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8883577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49fb616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860be173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c28fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f934d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42f29658",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "There are many different types of neural network architectures. The simplest and oldest architecture is the **multilayer perceptron (MLP)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d1821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cee21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
